Bootstrap: docker
From: python:3.11-slim

%post
    export DEBIAN_FRONTEND=noninteractive
    # Workarounds for rootless/fakeroot builds
    rm -rf /var/lib/apt/lists/*
    apt-get clean
    echo 'APT::Sandbox::User "root";' > /etc/apt/apt.conf.d/99sandboxroot || true
    apt-get update --allow-releaseinfo-change || echo "Warning: apt-get update failed"
    apt-get install -y \
        curl \
        wget \
        git \
        build-essential \
        || echo "Warning: Some packages may not have installed correctly"
    apt-get clean || true
    rm -rf /var/lib/apt/lists/* || true
    
    # Create requirements file and install dependencies
    cat > /tmp/requirements-test.txt << 'REQ_EOF'
pytest>=7.0.0
pytest-mock>=3.10.0
pytest-cov>=4.0.0
requests>=2.28.0
fastapi>=0.100.0
uvicorn[standard]>=0.23.0
httpx>=0.24.0
aiofiles>=23.0.0
REQ_EOF
    
    # Verify requirements file and install dependencies
    echo "Requirements file contents:"
    cat /tmp/requirements-test.txt
    pip install --no-cache-dir -r /tmp/requirements-test.txt

%environment
    export PYTHONPATH=/app/services/client/src:$PYTHONPATH

%runscript
    #!/bin/bash
    
    echo "Client Testing Container"
    echo "======================================"
    
    # Check if we're in the right directory
    if [ ! -f "/app/services/client/src/main.py" ]; then
        echo "Error: Must bind mount the project directory to /app"
        echo "Usage: apptainer run --bind /path/to/Benchmarking-AI-Factories:/app test-container.sif"
        exit 1
    fi
    
    echo "Running tests on compute node: $(hostname)"
    echo "========================================="
    
    # Load required modules 
    module load env/release/2023.1
    module load Apptainer/1.2.4-GCCcore-12.3.0
    
    cd /app
    
    #### UNIT TESTS ####
    # Run unit tests first (no services needed)
    echo ""
    echo "Running unit tests..."
    cd /app/services/client
    UNIT_LOG="tests/unit-test.log"
    python -m pytest tests/ -v --tb=short --ignore=tests/integration/ > "$UNIT_LOG" 2>&1
    UNIT_STATUS=$?
    if [ $UNIT_STATUS -eq 0 ]; then
        echo "Unit tests passed (see $UNIT_LOG)"
    else
        echo "Unit tests failed (see $UNIT_LOG)"
        echo "=== Unit Test Output ==="
        cat "$UNIT_LOG"
        exit 1
    fi

    #### INTEGRATION TESTS ####
    echo ""
    echo "Starting integration tests with client service..."
    cd /app/services/client
    
    # Clean up any existing endpoint file
    rm -f .client-service-endpoint
    
    # Start Client Service for integration tests
    export CLIENT_SERVICE_PORT=${CLIENT_SERVICE_PORT:-8001}
    export CLIENT_SERVICE_HOST=${CLIENT_SERVICE_HOST:-localhost}
    cd /app/services/client/src
    python -m uvicorn main:app --host 0.0.0.0 --port $CLIENT_SERVICE_PORT &
    CLIENT_SERVICE_PID=$!
    cd /app/services/client

    # Write endpoint file for test discovery
    CLIENT_SERVICE_ENDPOINT="http://$CLIENT_SERVICE_HOST:$CLIENT_SERVICE_PORT"
    echo "$CLIENT_SERVICE_ENDPOINT" > .client-service-endpoint

    # Wait for client service to be ready (max 60s)
    echo "Waiting for client service to start..."
    for i in {1..60}; do
        if curl -s $CLIENT_SERVICE_ENDPOINT/docs >/dev/null 2>&1; then
            echo "Client service is up and ready."
            break
        fi
        sleep 1
    done
    if ! curl -s $CLIENT_SERVICE_ENDPOINT/docs >/dev/null 2>&1; then
        echo "Client service failed to start within timeout."
        kill $CLIENT_SERVICE_PID 2>/dev/null || true
        exit 1
    fi
    echo "Client service started at: $CLIENT_SERVICE_ENDPOINT"
    
    # Start mock AI server for integration tests
    export AI_SERVER_PORT=${AI_SERVER_PORT:-8000}
    export AI_SERVER_HOST=${AI_SERVER_HOST:-localhost}
    cd /app/services/client/tests/integration
    python mock_ai_server.py --host $AI_SERVER_HOST --port $AI_SERVER_PORT &
    AI_SERVER_PID=$!
    cd /app/services/client
    
    # Wait for AI server to be ready
    AI_SERVER_ENDPOINT="http://$AI_SERVER_HOST:$AI_SERVER_PORT"
    echo "Waiting for mock AI server to start..."
    for i in {1..30}; do
        if curl -s $AI_SERVER_ENDPOINT/health >/dev/null 2>&1; then
            echo "Mock AI server is up and ready."
            break
        fi
        sleep 1
    done
    if ! curl -s $AI_SERVER_ENDPOINT/health >/dev/null 2>&1; then
        echo "Mock AI server failed to start within timeout."
        kill $CLIENT_SERVICE_PID 2>/dev/null || true
        kill $AI_SERVER_PID 2>/dev/null || true
        exit 1
    fi
    echo "Mock AI server started at: $AI_SERVER_ENDPOINT"
    
    # Export environment variables for integration tests
    export CLIENT_SERVICE_ADDR="$CLIENT_SERVICE_ENDPOINT"
    export AI_SERVER_ADDR="$AI_SERVER_ENDPOINT"
    
    # Run integration tests
    echo ""
    echo "Running integration tests..."
    INTEGRATION_LOG="tests/integration-test.log"
    python -m pytest tests/integration/ -v --tb=short > "$INTEGRATION_LOG" 2>&1
    INTEGRATION_STATUS=$?
    if [ $INTEGRATION_STATUS -eq 0 ]; then
        echo "Integration tests passed (see $INTEGRATION_LOG)"
    else
        echo "Integration tests failed (see $INTEGRATION_LOG)"
        echo "=== Integration Test Output ==="
        cat "$INTEGRATION_LOG"
        # Don't exit yet, cleanup first
    fi
    
    #### COVERAGE TESTS ####
    echo ""
    echo "Running coverage analysis..."
    COVERAGE_LOG="tests/coverage-test.log"
    python -m pytest tests/ --cov=src --cov-report=term --cov-report=html:tests/htmlcov --ignore=tests/integration/ > "$COVERAGE_LOG" 2>&1
    COVERAGE_STATUS=$?
    if [ $COVERAGE_STATUS -eq 0 ]; then
        echo "Coverage analysis completed (see $COVERAGE_LOG and tests/htmlcov/)"
        echo "Coverage summary:"
        tail -10 "$COVERAGE_LOG" | grep -E "(TOTAL|Name)"
    else
        echo "Coverage analysis failed (see $COVERAGE_LOG)"
    fi
    
    # Cleanup
    echo ""
    echo "Cleaning up..."
    kill $CLIENT_SERVICE_PID 2>/dev/null || true
    kill $AI_SERVER_PID 2>/dev/null || true
    sleep 5
    
    # Check overall test results
    if [ $UNIT_STATUS -eq 0 ] && [ $INTEGRATION_STATUS -eq 0 ]; then
        echo ""
        echo "All tests completed successfully!"
        echo "======================================"
        echo "Unit tests: PASSED"
        echo "Integration tests: PASSED"
        echo "Coverage analysis: $([ $COVERAGE_STATUS -eq 0 ] && echo "COMPLETED" || echo "FAILED")"
        echo ""
        echo "Ready to commit and push your changes!"
        exit 0
    else
        echo ""
        echo "Some tests failed!"
        echo "======================================"
        echo "Unit tests: $([ $UNIT_STATUS -eq 0 ] && echo "PASSED" || echo "FAILED")"
        echo "Integration tests: $([ $INTEGRATION_STATUS -eq 0 ] && echo "PASSED" || echo "FAILED")"
        echo ""
        echo "Check the output above for details."
        exit 1
    fi
    
%help
    This container runs tests for the Client Services.
    
    Usage:
        # Build the container
        cd /path/to/Benchmarking-AI-Factories/services/client
        apptainer build tests/test-container.sif tests/test-container.def
        
        # Run tests
        cd /path/to/Benchmarking-AI-Factories
        apptainer run --bind $(pwd):/app services/client/tests/test-container.sif
    
    What it does:
        1. Runs unit tests (isolated with mocks)
        2. Starts client service and mock AI server
        3. Runs integration tests
        4. Generates coverage reports
        5. Cleans up all services
    
    Test logs are saved to:
        - tests/unit-test.log
        - tests/integration-test.log  
        - tests/coverage-test.log
        - tests/htmlcov/ (coverage HTML report)