name: vllm
category: inference
description: "vLLM high-performance inference server for large language models"
version: "0.2.0"
image: "vllm.sif"
container_def: "vllm.def"
ports:
  - 8001
environment:
  VLLM_HOST: "0.0.0.0"
  VLLM_PORT: "8001"
  VLLM_MODEL: "Qwen/Qwen2.5-0.5B-Instruct"  # Default model, can be overridden in config
  VLLM_WORKDIR: "/workspace"
  VLLM_LOGGING_LEVEL: "INFO"
resources: # Default resources, can be overridden in config
  nodes: "1"
  cpu: "2" #"8"
  memory: "32G"
  time_limit: 15
  gpu: "1"
distributed:
  # Enable distributed launches: torchrun + srun will be used by SlurmDeployer.
  # nproc_per_node: number of local processes per node (typically GPUs per node)
  nproc_per_node: 1
  # rendezvous port (open on master node)
  master_port: 29500
  # rendezvous backend
  rdzv_backend: c10d