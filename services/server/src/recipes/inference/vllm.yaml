name: vllm
category: inference
description: "vLLM high-performance inference server for large language models"
version: "0.2.0"
image: "vllm.sif"
container_def: "vllm.def"
ports:
  - 8000
environment:
  CUDA_VISIBLE_DEVICES: "0"
  VLLM_HOST: "0.0.0.0"
  VLLM_PORT: "8001"
  VLLM_MODEL: "Qwen/Qwen2.5-0.5B-Instruct"  # Default model, can be overridden in config
resources: # Default resources, can be overridden in config
  nodes: "1"
  cpu: "2" #"8"
  memory: "32G"
  time_limit: 15
  gpu: "1"