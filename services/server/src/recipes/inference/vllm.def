Bootstrap: docker
From: vllm/vllm-openai:v0.11.2

%labels
    Description "Apptainer container for vLLM OpenAI-compatible API"
    Version latest

%post
    apt-get update && apt-get install -y git wget curl vim \
        && apt-get clean && rm -rf /var/lib/apt/lists/*

    mkdir -p /workspace /workspace/huggingface_cache /workspace/metrics
    chmod -R 777 /workspace

%environment
    export PATH=/usr/local/bin:$PATH
    export PYTHONUNBUFFERED=1
    export VLLM_WORKDIR=/workspace
    export HF_HOME=$HOME/.cache/huggingface
    export XDG_CACHE_HOME=/workspace/.cache
    export TORCH_COMPILE_CACHE=/workspace/.cache/vllm/torch_compile_cache
    # Prevent Ray from trying to cleanup (it's not needed with external_launcher)
    export RAY_DEDUP_LOGS=0
    export RAY_IGNORE_UNHANDLED_ERRORS=1

%runscript
    echo "Starting vLLM OpenAI-compatible API..."
    echo "Model: ${VLLM_MODEL:-Qwen/Qwen2.5-0.5B-Instruct}"
    echo "Endpoint: ${VLLM_HOST:-0.0.0.0}:${VLLM_PORT:-8001}"
    
    VLLM_CMD="python3 -m vllm.entrypoints.openai.api_server"
    VLLM_CMD="${VLLM_CMD} --host ${VLLM_HOST:-0.0.0.0}"
    VLLM_CMD="${VLLM_CMD} --port ${VLLM_PORT:-8001}"
    VLLM_CMD="${VLLM_CMD} --model ${VLLM_MODEL:-Qwen/Qwen2.5-0.5B-Instruct}"

    [ -n "${VLLM_MAX_MODEL_LEN}" ] && VLLM_CMD="${VLLM_CMD} --max-model-len ${VLLM_MAX_MODEL_LEN}"
    [ -n "${VLLM_TENSOR_PARALLEL_SIZE}" ] && VLLM_CMD="${VLLM_CMD} --tensor-parallel-size ${VLLM_TENSOR_PARALLEL_SIZE}"
    [ -n "${VLLM_GPU_MEMORY_UTILIZATION}" ] && VLLM_CMD="${VLLM_CMD} --gpu-memory-utilization ${VLLM_GPU_MEMORY_UTILIZATION}"
    [ -n "${MASTER_ADDR}" ] && [ -n "${MASTER_PORT}" ] && \
        VLLM_CMD="${VLLM_CMD} --distributed-init-method tcp://${MASTER_ADDR}:${MASTER_PORT}"

    echo "Executing: ${VLLM_CMD}"
    exec ${VLLM_CMD}
