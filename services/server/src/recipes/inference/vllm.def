Bootstrap: docker
From: vllm/vllm-openai:nightly-da4455609d9e29abebc15f6c598dfd772c2b6513

%labels
    Description "Apptainer container for vLLM OpenAI-compatible API (nightly build)"
    Version nightly-da4455609d9e29abebc15f6c598dfd772c2b6513

%post
    apt-get update && apt-get install -y \
        git wget curl vim \
        && apt-get clean && rm -rf /var/lib/apt/lists/*

    # create workspace
    mkdir -p /workspace
    # create huggingface cache directory inside workspace and set permissive permissions
    mkdir -p /workspace/huggingface_cache
    chmod -R 777 /workspace
    chmod -R 777 /workspace/huggingface_cache

%environment
    # Setup environment variables
    export PATH=/usr/local/bin:$PATH
    export PYTHONUNBUFFERED=1
    export VLLM_WORKDIR=/workspace
    # Explicit Hugging Face cache locations
    export HF_HOME=/workspace/huggingface_cache
    # Explicit pytorch and vllm cache locations
    export XDG_CACHE_HOME=/workspace/.cache
    export TORCH_COMPILE_CACHE=/workspace/.cache/vllm/torch_compile_cache

%runscript
    echo "Starting vLLM OpenAI-compatible API..."
    # Respect environment overrides for host/port so the recipe or runtime can control binding
    : "Using VLLM_HOST=${VLLM_HOST:-0.0.0.0} VLLM_PORT=${VLLM_PORT:-8001}"
    exec python3 -m vllm.entrypoints.openai.api_server --host "${VLLM_HOST:-0.0.0.0}" --port "${VLLM_PORT:-8001}"

%startscript
    # Used with `apptainer instance start`
    # Respect environment overrides for host/port
    exec python3 -m vllm.entrypoints.openai.api_server --host "${VLLM_HOST:-0.0.0.0}" --port "${VLLM_PORT:-8001}"
