name: vllm-replicas
category: inference
description: "vLLM with multiple replicas - flexible GPU allocation per replica"
version: "1.0.0"
image: "vllm.sif"
container_def: "vllm.def"

environment:
  VLLM_HOST: "0.0.0.0"
  VLLM_MODEL: "Qwen/Qwen2.5-0.5B-Instruct"
  VLLM_WORKDIR: "/workspace"
  VLLM_LOGGING_LEVEL: "INFO"
  VLLM_GPU_MEMORY_UTILIZATION: "0.9"

resources:  # Per node (not per replica)
  nodes: "1"  # Number of nodes to allocate
  cpu: "8"    # CPUs per node
  memory: "64G"  # Memory per node
  time_limit: 60  # 60 minutes = 1 hour
  gpu: "4"    # Total GPUs per node

# Replica group configuration
# System calculates: replicas_per_node = gpu / gpu_per_replica
gpu_per_replica: 1  # Each replica uses 1 GPU (data parallel)
base_port: 8001     # First replica uses 8001, second uses 8002, etc.

# Notes:
# - With gpu=4 and gpu_per_replica=1, you get 4 replicas per node
# - All replicas run in a single SLURM job per node
# - Each replica listens on base_port + replica_index
# - Load balancing distributes requests across all replicas
