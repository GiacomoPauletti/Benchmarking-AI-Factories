name: triton
category: inference
description: "NVIDIA Triton Inference Server for optimized model serving"
version: "23.10"
image: "nvcr.io/nvidia/tritonserver:23.10-py3"
ports:
  - 8000
  - 8001
  - 8002
environment:
  TRITON_LOG_VERBOSE: "1"
  TRITON_MODEL_REPOSITORY: "/models"
resources:
  cpu: "8"
  memory: "32Gi"
  gpu: "2"
  gpu_memory: "80Gi"
deployment_config:
  replicas: 1
  model_repository: "/models"
  strict_model_config: false
  model_control_mode: "poll"
health_check:
  endpoint: "/v2/health/ready"
  interval: 30
  timeout: 10
  retries: 3