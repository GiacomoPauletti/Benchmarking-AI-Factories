name: vllm_dummy
category: inference
description: "Simple dummy vLLM API server for testing (mock responses)"
version: "0.2.0"
image: "vllm_dummy.sif"
container_def: "vllm_dummy.def"
ports:
  - 8000
environment:
  VLLM_HOST: "0.0.0.0"
  VLLM_PORT: "8000"
resources:
  cpu: "8"
  memory: "16G"
deployment_config:
  execution_type: "slurm"
  command: ""
  model_path: "/models"
  tensor_parallel_size: 1
  max_model_len: 4096
health_check:
  endpoint: "/health"
  interval: 30
  timeout: 10
  retries: 3