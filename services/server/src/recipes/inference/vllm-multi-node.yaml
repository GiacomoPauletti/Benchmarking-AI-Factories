name: vllm-multi-node
category: inference
description: "vLLM multi-node inference server with tensor parallelism across nodes"
version: "0.2.0"
image: "vllm.sif"
container_def: "vllm.def"
ports:
  - 8001
environment:
  VLLM_HOST: "0.0.0.0"
  VLLM_PORT: "8001"
  VLLM_MODEL: "Qwen/Qwen2.5-3B-Instruct"  # 3B model with 16 attention heads (divisible by 8)
  VLLM_WORKDIR: "/workspace"
  VLLM_LOGGING_LEVEL: "INFO"
resources:
  nodes: "2"  # Multi-node setup
  cpu: "32"   # 16 CPUs per node recommended for multi-GPU
  memory: "64G"
  time_limit: 15
  gpu: "4"    # 4 GPUs per node (8 total across 2 nodes)
distributed:
  # Number of processes per node (typically matches GPUs per node)
  nproc_per_node: 4
  # Rendezvous port for coordination
  master_port: 29500
  # Rendezvous backend
  rdzv_backend: c10d
