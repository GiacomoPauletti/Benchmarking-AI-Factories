# Server feature development workflow:
git checkout feature/server-dev
git checkout -b feature/server/new-feature-name
git add .
git commit -m "Add new server feature"
git push origin feature/server/new-feature-name
gh pr create --title "Add new server feature" --body "Description" --base feature/server-dev


#### SETUP ####

#### RUN SERVER LOCALLY ####
# Option 1: Use launcher script 
~/Documents/Career_Academics/EUMaster4HPC/Courses/Semester_3/challenge/Benchmarking-AI-Factories/services/server/launch_local.sh

# Option 2: Direct docker compose (from project root)
cd ~/Documents/Career_Academics/EUMaster4HPC/Courses/Semester_3/challenge/Benchmarking-AI-Factories
docker compose up server

# use the shell script
~/Documents/Career_Academics/EUMaster4HPC/Courses/Semester_3/challenge/Benchmarking-AI-Factories/services/server/server-shell.sh

#### RUN TESTS ####
# Run all tests in Docker container (fast, local)
cd ~/Documents/Career_Academics/EUMaster4HPC/Courses/Semester_3/challenge/Benchmarking-AI-Factories
./services/server/run-tests.sh

#### CLEAN UP ####
# Clear logs folder (ignores files that can't be removed)
rm -f ~/Documents/Career_Academics/EUMaster4HPC/Courses/Semester_3/challenge/Benchmarking-AI-Factories/services/server/logs/* 2>/dev/null || true

# Alternative: Clear from anywhere
cd ~/Documents/Career_Academics/EUMaster4HPC/Courses/Semester_3/challenge/Benchmarking-AI-Factories
rm -f services/server/logs/* 2>/dev/null || true

# Clean up Qdrant data directories on MeluXina (removes ALL vector-db data)
ssh u103056@login.lxp.lu "rm -rf /project/home/p200981/u103056/Benchmarking-AI-Factories/services/server/qdrant_data*"

# Clean up specific Qdrant instance data (replace JOB_ID with actual job ID)
ssh u103056@login.lxp.lu "rm -rf /project/home/p200981/u103056/Benchmarking-AI-Factories/services/server/qdrant_data_JOB_ID"

# Remove orphaned lock files (if Qdrant crashed)
ssh u103056@login.lxp.lu "find /project/home/p200981/u103056/Benchmarking-AI-Factories/services/server/qdrant_data* -name 'LOCK' -delete"

#### API TESTING (Local Server) ####
# Server runs on http://localhost:8001

# Interactive API docs
http://localhost:8001/docs

# Alternative docs
http://localhost:8001/redoc

# Export OpenAPI spec
cd ~/Documents/Career_Academics/EUMaster4HPC/Courses/Semester_3/challenge/Benchmarking-AI-Factories
python3 services/server/tools/export_openapi_html.py

#### CLIENT CLI COMMANDS ####
# Create vLLM with DEFAULT model (Qwen/Qwen2.5-0.5B-Instruct)
create inference/vllm

# List all vLLM services
vllm list

# Check which models are loaded in a service
vllm models 3664648

# Send prompts 
prompt 3664648 'Tell me a joke about programming'

# Create vecdb qdrant service
create vector-db/qdrant

# List all vecdb services
vectordb list

# List collections in a vector-db service
vectordb collections 3664650

# Get detailed info about a collection
vectordb info 3664650 my_documents

# Create a new collection (384-dim vectors with Cosine similarity)
vectordb create 3664650 my_documents 3 Cosine

# Create collection with Euclidean distance
vectordb create 3664650 embeddings 30 Euclid

# Insert/update points (vectors with payloads) into a collection
vectordb upsert 3664650 my_documents '[{"id":1,"vector":[0.1,0.2,0.3],"payload":{"text":"hello"}}]'

# Search for similar vectors
vectordb search 3664650 my_documents '[0.15,0.25,0.35]' 5
vectordb search 3664650 my_documents '[4.15,-0.25,7.35]' 5

# Delete a collection
vectordb delete 3664650 my_documents


#### API CURL EXAMPLES ####
# Create vLLM with custom model AND custom resources
curl -X POST http://localhost:8001/api/v1/services \
  -H "Content-Type: application/json" \
  -d '{
    "recipe_name": "inference/vllm",
    "config": {
      "environment": {
        "VLLM_MODEL": "gpt2"
      },
      "resources": {
        "nodes": 1,
        "cpu": "8",
        "memory": "64G",
        "time_limit": 120,
        "gpu": "1"
      }
    }
  }'

# Create vLLM service with CUSTOM model (using API, with ALL options explicitly set)
curl -X POST http://localhost:8001/api/v1/services \
  -H "Content-Type: application/json" \
  -d '{
    "recipe_name": "inference/vllm",
    "config": {
      "environment": {
        "VLLM_MODEL": "gpt2",
        "VLLM_HOST": "0.0.0.0",
        "VLLM_PORT": "8001",
        "VLLM_MAX_MODEL_LEN": "2048",
        "VLLM_TENSOR_PARALLEL_SIZE": "1",
        "VLLM_GPU_MEMORY_UTILIZATION": "0.9",
        "CUDA_VISIBLE_DEVICES": "0"
      },
      "resources": {
        "nodes": 1,
        "cpu": "2",
        "memory": "32G",
        "time_limit": 15,
        "gpu": "1"
      }
    }
  }'

# Create CPU-only vLLM (no GPU)
curl -X POST http://localhost:8001/api/v1/services \
  -H "Content-Type: application/json" \
  -d '{
    "recipe_name": "inference/vllm",
    "config": {
      "environment": {
        "VLLM_MODEL": "gpt2"
      },
      "resources": {
        "nodes": 1,
        "cpu": "4",
        "memory": "16G",
        "gpu": null
      }
    }
  }'


# You can now specify any model when creating the service:
# - Default model (from recipe): Qwen/Qwen2.5-0.5B-Instruct
# - Override with config: gpt2, facebook/opt-125m, microsoft/phi-2, etc.
# - Model must be available on HuggingFace or cached locally
# - See: services/server/docs/vllm-custom-models.md for full guide