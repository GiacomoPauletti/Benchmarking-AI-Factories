# Server feature development workflow:
git checkout feature/server-dev
git checkout -b feature/server/new-feature-name
git add .
git commit -m "Add new server feature"
git push origin feature/server/new-feature-name
gh pr create --title "Add new server feature" --body "Description" --base feature/server-dev


#### SETUP ####

#### RUN SERVER LOCALLY ####
# Option 1: Use launcher script 
~/Documents/Career_Academics/EUMaster4HPC/Courses/Semester_3/challenge/Benchmarking-AI-Factories/services/server/launch_local.sh

# Option 2: Direct docker compose (from project root)
cd ~/Documents/Career_Academics/EUMaster4HPC/Courses/Semester_3/challenge/Benchmarking-AI-Factories
docker compose up server

# use the shell script
~/Documents/Career_Academics/EUMaster4HPC/Courses/Semester_3/challenge/Benchmarking-AI-Factories/services/server/server-shell.sh

#### RUN TESTS ####
# Run all tests in Docker container (fast, local)
cd ~/Documents/Career_Academics/EUMaster4HPC/Courses/Semester_3/challenge/Benchmarking-AI-Factories
./services/server/run-tests.sh

# Or manually run specific tests
docker compose exec server pip install pytest pytest-mock
docker compose exec server python -m pytest tests/test_api.py -v          # Unit tests only
docker compose exec server python -m pytest tests/test_integration.py -v  # Integration tests
docker compose exec server python -m pytest tests/ -v                     # All tests

#### API TESTING (Local Server) ####
# Server runs on http://localhost:8001

# Interactive API docs
http://localhost:8001/docs

# Alternative docs
http://localhost:8001/redoc

# Export OpenAPI spec
cd ~/Documents/Career_Academics/EUMaster4HPC/Courses/Semester_3/challenge/Benchmarking-AI-Factories
python3 services/server/tools/export_openapi_html.py

#### CLIENT CLI COMMANDS ####
# Create vLLM with DEFAULT model (Qwen/Qwen2.5-0.5B-Instruct)
create inference/vllm

# List all vLLM services
vllm list

# Check which models are loaded in a service
vllm models 3652057

# Send prompts 
prompt 3652098 'Tell me a joke about programming'

#### API CURL EXAMPLES ####
# Create vLLM with custom model AND custom resources
curl -X POST http://localhost:8001/api/v1/services \
  -H "Content-Type: application/json" \
  -d '{
    "recipe_name": "inference/vllm",
    "config": {
      "environment": {
        "VLLM_MODEL": "gpt2"
      },
      "resources": {
        "nodes": 1,
        "cpu": "8",
        "memory": "64G",
        "time_limit": 120,
        "gpu": "1"
      }
    }
  }'

# Create vLLM service with CUSTOM model (using API, with ALL options explicitly set)
curl -X POST http://localhost:8001/api/v1/services \
  -H "Content-Type: application/json" \
  -d '{
    "recipe_name": "inference/vllm",
    "config": {
      "environment": {
        "VLLM_MODEL": "gpt2",
        "VLLM_HOST": "0.0.0.0",
        "VLLM_PORT": "8001",
        "VLLM_MAX_MODEL_LEN": "2048",
        "VLLM_TENSOR_PARALLEL_SIZE": "1",
        "VLLM_GPU_MEMORY_UTILIZATION": "0.9",
        "CUDA_VISIBLE_DEVICES": "0"
      },
      "resources": {
        "nodes": 1,
        "cpu": "2",
        "memory": "32G",
        "time_limit": 15,
        "gpu": "1"
      }
    }
  }'

# Create CPU-only vLLM (no GPU)
curl -X POST http://localhost:8001/api/v1/services \
  -H "Content-Type: application/json" \
  -d '{
    "recipe_name": "inference/vllm",
    "config": {
      "environment": {
        "VLLM_MODEL": "gpt2"
      },
      "resources": {
        "nodes": 1,
        "cpu": "4",
        "memory": "16G",
        "gpu": null
      }
    }
  }'


# You can now specify any model when creating the service:
# - Default model (from recipe): Qwen/Qwen2.5-0.5B-Instruct
# - Override with config: gpt2, facebook/opt-125m, microsoft/phi-2, etc.
# - Model must be available on HuggingFace or cached locally
# - See: services/server/docs/vllm-custom-models.md for full guide