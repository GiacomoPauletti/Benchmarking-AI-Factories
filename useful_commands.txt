# Server feature development workflow:
git checkout feature/server-dev
git checkout -b feature/server/new-feature-name
git add .
git commit -m "Add new server feature"
git push origin feature/server/new-feature-name
gh pr create --title "Add new server feature" --body "Description" --base feature/server-dev


#### RUN TESTS ####
cd "$HOME/Benchmarking-AI-Factories" && ./services/server/run-tests.sh

#### ENDPOINTS Testing ####
# Generate docs
ssh -L 8001:mel0533:8001 $USER@login03
http://localhost:8001/docs
http://localhost:8001/redoc
python3 services/server/tools/export_openapi_html.py

# Remove existing container if rebuild is wanted
# Remove existing containers and rebuild server
cd "$HOME/Benchmarking-AI-Factories/services/server" && rm -f server.sif && cd "$HOME/Benchmarking-AI-Factories" && ./services/server/launch_server.sh

cd "$HOME/Benchmarking-AI-Factories" && ./services/server/launch_server.sh

cd "$HOME/Benchmarking-AI-Factories" && ./services/server/server-shell.sh

# Create vLLM with DEFAULT model (Qwen/Qwen2.5-0.5B-Instruct)
create inference/vllm

# List all vLLM services
vllm list

# Check which models are loaded in a service
vllm models 3643966

# Send prompts 
prompt 3643982 'Tell me a joke about programming'

# Create vLLM with custom model AND custom resources
curl -X POST http://mel0378:8001/api/v1/services \
  -H "Content-Type: application/json" \
  -d '{
    "recipe_name": "inference/vllm",
    "config": {
      "environment": {
        "VLLM_MODEL": "gpt2"
      },
      "resources": {
        "nodes": 1,
        "cpu": "8",
        "memory": "64G",
        "time_limit": 120,
        "gpu": "1"
      }
    }
  }'

# Create vLLM service with CUSTOM model (using API, with ALL options explicitly set)
curl -X POST http://mel0015:8001/api/v1/services \
  -H "Content-Type: application/json" \
  -d '{
    "recipe_name": "inference/vllm",
    "config": {
      "environment": {
        "VLLM_MODEL": "gpt2",
        "VLLM_HOST": "0.0.0.0",
        "VLLM_PORT": "8001",
        "VLLM_MAX_MODEL_LEN": "2048",
        "VLLM_TENSOR_PARALLEL_SIZE": "1",
        "VLLM_GPU_MEMORY_UTILIZATION": "0.9",
        "CUDA_VISIBLE_DEVICES": "0"
      },
      "resources": {
        "nodes": 1,
        "cpu": "2",
        "memory": "32G",
        "time_limit": 15,
        "gpu": "1"
      }
    }
  }'

# Create CPU-only vLLM (no GPU)
curl -X POST http://mel0015:8001/api/v1/services \
  -H "Content-Type: application/json" \
  -d '{
    "recipe_name": "inference/vllm",
    "config": {
      "environment": {
        "VLLM_MODEL": "gpt2"
      },
      "resources": {
        "nodes": 1,
        "cpu": "4",
        "memory": "16G",
        "gpu": null
      }
    }
  }'


# You can now specify any model when creating the service:
# - Default model (from recipe): Qwen/Qwen2.5-0.5B-Instruct
# - Override with config: gpt2, facebook/opt-125m, microsoft/phi-2, etc.
# - Model must be available on HuggingFace or cached locally
# - See: services/server/docs/vllm-custom-models.md for full guide