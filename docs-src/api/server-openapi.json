{"openapi":"3.1.0","info":{"title":"AI Factory Server Service","description":"SLURM + Apptainer orchestration for AI workloads","version":"1.0.0"},"paths":{"/":{"get":{"summary":"Root","description":"Root endpoint with service information.","operationId":"root__get","responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}}}}},"/health":{"get":{"summary":"Health","description":"Health check endpoint.","operationId":"health_health_get","responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}}}}},"/api/v1/metrics/vllm/{job_id}":{"get":{"tags":["metrics"],"summary":"Proxy Vllm Metrics","description":"Proxy /metrics endpoint from vLLM service running in SLURM.\n\nThis allows Prometheus to scrape vLLM metrics without direct access\nto the compute node.\n\n**Path Parameters:**\n- `job_id`: SLURM job ID of the vLLM service\n\n**Returns:**\n- Prometheus-formatted metrics from vLLM (200 OK)\n- 503 Service Unavailable if service exists but isn't running yet\n- 404 Not Found if service doesn't exist\n\n**Example:**\n```\nGET /metrics/vllm/3691724\n```","operationId":"proxy_vllm_metrics_api_v1_metrics_vllm__job_id__get","parameters":[{"name":"job_id","in":"path","required":true,"schema":{"type":"string","title":"Job Id"}}],"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}}},"/api/v1/metrics/qdrant/{job_id}":{"get":{"tags":["metrics"],"summary":"Proxy Qdrant Metrics","description":"Proxy /metrics endpoint from Qdrant service running in SLURM.\n\nThis allows Prometheus to scrape Qdrant metrics without direct access\nto the compute node.\n\n**Path Parameters:**\n- `job_id`: SLURM job ID of the Qdrant service\n\n**Returns:**\n- Prometheus-formatted metrics from Qdrant (200 OK)\n- 503 Service Unavailable if service exists but isn't running yet\n- 404 Not Found if service doesn't exist\n\n**Example:**\n```\nGET /metrics/qdrant/3691725\n```","operationId":"proxy_qdrant_metrics_api_v1_metrics_qdrant__job_id__get","parameters":[{"name":"job_id","in":"path","required":true,"schema":{"type":"string","title":"Job Id"}}],"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}}},"/api/v1/metrics/service/{job_id}":{"get":{"tags":["metrics"],"summary":"Proxy Service Metrics","description":"Generic metrics proxy for any service running in SLURM.\n\nAutomatically detects the service type and proxies its /metrics endpoint.\n\n**Path Parameters:**\n- `job_id`: SLURM job ID of the service\n\n**Returns:**\n- Prometheus-formatted metrics from the service (200 OK)\n- 503 Service Unavailable if service exists but isn't running yet\n- 404 Not Found if service doesn't exist\n\n**Example:**\n```\nGET /metrics/service/3691724\n```","operationId":"proxy_service_metrics_api_v1_metrics_service__job_id__get","parameters":[{"name":"job_id","in":"path","required":true,"schema":{"type":"string","title":"Job Id"}}],"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}}},"/api/v1/metrics/all":{"get":{"tags":["metrics"],"summary":"Get All Services Metrics","description":"Get a list of all running services with their metrics endpoints.\n\nThis is useful for Prometheus service discovery.\n\n**Returns:**\n- List of services with their metrics proxy URLs\n\n**Example Response:**\n```json\n{\n  \"services\": [\n    {\n      \"job_id\": \"3691724\",\n      \"name\": \"vllm-service\",\n      \"metrics_url\": \"http://server:8001/metrics/vllm/3691724\"\n    },\n    {\n      \"job_id\": \"3691725\",\n      \"name\": \"qdrant-service\",\n      \"metrics_url\": \"http://server:8001/metrics/qdrant/3691725\"\n    }\n  ]\n}\n```","operationId":"get_all_services_metrics_api_v1_metrics_all_get","responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}}}}},"/api/v1/services":{"post":{"summary":"Create and start a new service","description":"Create and start a new service using SLURM + Apptainer.\n\nThis endpoint submits a job to the SLURM cluster using a predefined recipe template.\nThe service will be containerized using Apptainer and scheduled on compute nodes.\n\n**Request Body:**\n- `recipe_name` (required): Path to the recipe (e.g., \"inference/vllm\", \"inference/vllm_dummy\")\n- `config` (optional): Configuration object with:\n    - **SLURM resource requirements** (all optional, override recipe defaults):\n        - `nodes`: Number of compute nodes (default: 1)\n        - `resources`: Resource overrides object:\n            - `cpu`: Number of CPUs (e.g., \"2\", \"8\")\n            - `memory`: Memory per CPU (e.g., \"8G\", \"16G\", \"32G\")\n            - `time_limit`: Time limit in minutes (e.g., 15, 60, 120)\n            - `gpu`: GPU allocation (e.g., \"1\", \"2\", null for CPU-only)\n    - **Environment variables** (all optional, override recipe defaults):\n        - `environment`: Environment variable overrides:\n            - `VLLM_MODEL`: Model to load (e.g., \"gpt2\", \"Qwen/Qwen2.5-0.5B-Instruct\")\n            - `VLLM_MAX_MODEL_LEN`: Max sequence length\n            - `VLLM_GPU_MEMORY_UTILIZATION`: GPU memory fraction (0.0-1.0)\n            - Any other environment variables supported by the container\n\n**Examples:**\n\nSimple creation with defaults:\n```json\n{\n  \"recipe_name\": \"inference/vllm\"\n}\n```\n\nCustom model:\n```json\n{\n  \"recipe_name\": \"inference/vllm\",\n  \"config\": {\n    \"environment\": {\n      \"VLLM_MODEL\": \"gpt2\"\n    }\n  }\n}\n```\n\nCustom model + resources:\n```json\n{\n  \"recipe_name\": \"inference/vllm\",\n  \"config\": {\n    \"nodes\": 1,\n    \"environment\": {\n      \"VLLM_MODEL\": \"meta-llama/Llama-2-7b-chat-hf\"\n    },\n    \"resources\": {\n      \"cpu\": \"8\",\n      \"memory\": \"64G\",\n      \"time_limit\": 120,\n      \"gpu\": \"1\"\n    }\n  }\n}\n```\n\n**Returns:**\n- Service object with `id` (SLURM job ID), `name`, `status`, `recipe_name`, `config`, and `created_at`\n\n**Status Values:**\n- `pending`: Job queued in SLURM\n- `building`: Apptainer container image being built\n- `starting`: Container started, application initializing\n- `running`: Service fully operational and ready\n- `completed`: Service finished successfully\n- `failed`: Service encountered an error\n- `cancelled`: Service was stopped by user","operationId":"create_service_api_v1_services_post","requestBody":{"required":true,"content":{"application/json":{"schema":{"allOf":[{"$ref":"#/components/schemas/ServiceRequest"}],"examples":{"simple":{"summary":"Create a basic vLLM service","value":{"recipe_name":"inference/vllm","config":{"nodes":1,"cpus":2,"memory":"8G","time":"00:30:00"}}}},"title":"Request"}}}},"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{"$ref":"#/components/schemas/ServiceResponse"}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}},"get":{"summary":"List Services","description":"List all services managed by this server.\n\nReturns all services that were started through this API. Does not include\nunrelated SLURM jobs (e.g., interactive sessions, other users' jobs).\n\n**Returns:**\n- Array of service objects, each containing:\n    - `id`: SLURM job ID (used as service identifier)\n    - `name`: Service name (derived from recipe)\n    - `status`: Current status (pending/building/starting/running/completed/failed/cancelled)\n    - `recipe_name`: Recipe used to create the service\n    - `config`: Resource configuration (nodes, cpus, memory, time, etc.)\n    - `created_at`: Service creation timestamp\n\n**Example Response:**\n```json\n[\n  {\n    \"id\": \"3642874\",\n    \"name\": \"vllm-service\",\n    \"status\": \"running\",\n    \"recipe_name\": \"inference/vllm\",\n    \"config\": {\"nodes\": 1, \"cpus\": 4, \"memory\": \"16G\"},\n    \"created_at\": \"2025-10-14T10:30:00\"\n  }\n]\n```","operationId":"list_services_api_v1_services_get","responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{"type":"array","items":{"$ref":"#/components/schemas/ServiceResponse"},"title":"Response List Services Api V1 Services Get"}}}}}}},"/api/v1/services/{service_id}":{"get":{"summary":"Get Service","description":"Get detailed information about a specific service.\n\n**Path Parameters:**\n- `service_id`: The SLURM job ID of the service (obtained from create_service or list_services)\n\n**Returns:**\n- Service object with current status and configuration details\n\n**Errors:**\n- 404: Service not found (not managed by this server)\n\n**Example:**\n- GET `/api/v1/services/3642874`","operationId":"get_service_api_v1_services__service_id__get","parameters":[{"name":"service_id","in":"path","required":true,"schema":{"type":"string","title":"Service Id"}}],"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{"$ref":"#/components/schemas/ServiceResponse"}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}},"delete":{"summary":"Stop Service","description":"DEPRECATED: Use POST /api/v1/services/{service_id}/status instead.\n\nStop a running service by cancelling its SLURM job.\n\n**DEPRECATION NOTICE:** This endpoint is deprecated in favor of the POST endpoint which\nbetter preserves service metadata for post-mortem analysis and Grafana integration.\nUse `POST /api/v1/services/{service_id}/status` with `{\"status\": \"cancelled\"}` instead.\n\nThis endpoint cancels the SLURM job associated with the service, which will:\n1. Terminate the running container\n2. Free up allocated compute resources\n3. Mark the service as cancelled in SLURM\n\n**Path Parameters:**\n- `service_id`: The SLURM job ID of the service to stop\n\n**Returns:**\n- Success message with service ID\n\n**Errors:**\n- 404: Service not found or already stopped\n\n**Example (DEPRECATED):**\n- DELETE `/api/v1/services/3642874`\n\n**Recommended Alternative:**\n```bash\ncurl -X POST http://localhost:8001/api/v1/services/3642874/status \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"status\": \"cancelled\"}'\n```\n\n**Note:** This operation is immediate and cannot be undone. The service will be terminated gracefully.","operationId":"stop_service_api_v1_services__service_id__delete","parameters":[{"name":"service_id","in":"path","required":true,"schema":{"type":"string","title":"Service Id"}}],"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}}},"/api/v1/service-groups":{"get":{"summary":"List Service Groups","description":"List all service groups.\n\nReturns summary information for all service groups (collections of replicas).\n\n**Returns:**\n```json\n[\n  {\n    \"id\": \"sg-ba5f6e2462fb\",\n    \"type\": \"replica_group\",\n    \"recipe_name\": \"inference/vllm-replicas\",\n    \"total_replicas\": 4,\n    \"healthy_replicas\": 3,\n    \"starting_replicas\": 1,\n    \"pending_replicas\": 0,\n    \"failed_replicas\": 0,\n    \"created_at\": \"2025-11-10T12:00:00\"\n  },\n  ...\n]\n```\n\n**Example:**\n- GET `/api/v1/service-groups`","operationId":"list_service_groups_api_v1_service_groups_get","responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}}}}},"/api/v1/service-groups/{group_id}":{"get":{"summary":"Get Service Group","description":"Get detailed information about a service group and all its replicas.\n\nService groups are collections of replica services that share a common group_id.\nThis endpoint aggregates information from all replicas in the group.\n\n**Path Parameters:**\n- `group_id`: The service group ID (e.g., \"sg-ba5f6e2462fb\")\n\n**Returns:**\n```json\n{\n  \"id\": \"sg-ba5f6e2462fb\",\n  \"type\": \"replica_group\",\n  \"replicas\": [\n    {\n      \"id\": \"3713894:8001\",\n      \"name\": \"vllm-replicas-3713894-replica-0\",\n      \"status\": \"running\",\n      \"port\": 8001,\n      \"gpu_id\": 0,\n      \"replica_index\": 0,\n      \"job_id\": \"3713894\"\n    },\n    ...\n  ],\n  \"total_replicas\": 4,\n  \"healthy_replicas\": 3,\n  \"starting_replicas\": 1,\n  \"failed_replicas\": 0,\n  \"recipe_name\": \"inference/vllm-replicas\",\n  \"base_port\": 8001,\n  \"node_jobs\": [\n    {\n      \"job_id\": \"3713894\",\n      \"node_index\": 0,\n      \"replicas\": [...]\n    }\n  ]\n}\n```\n\n**Errors:**\n- 404: Service group not found (no services with this group_id)\n\n**Example:**\n- GET `/api/v1/service-groups/sg-ba5f6e2462fb`","operationId":"get_service_group_api_v1_service_groups__group_id__get","parameters":[{"name":"group_id","in":"path","required":true,"schema":{"type":"string","title":"Group Id"}}],"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}},"delete":{"summary":"Stop Service Group","description":"Stop all replicas in a service group.\n\nThis endpoint cancels all SLURM jobs associated with the service group,\nstopping all replicas at once.\n\n**Path Parameters:**\n- `group_id`: The service group ID (e.g., \"sg-ba5f6e2462fb\")\n\n**Returns:**\n```json\n{\n  \"message\": \"Service group sg-ba5f6e2462fb stopped successfully\",\n  \"group_id\": \"sg-ba5f6e2462fb\",\n  \"replicas_stopped\": 4\n}\n```\n\n**Errors:**\n- 404: Service group not found\n- 500: Failed to stop one or more replicas\n\n**Example:**\n- DELETE `/api/v1/service-groups/sg-ba5f6e2462fb`\n\n**Note:** This operation stops all replicas in the group. Individual replicas\ncannot be stopped separately - the entire group is managed as a unit.","operationId":"stop_service_group_api_v1_service_groups__group_id__delete","parameters":[{"name":"group_id","in":"path","required":true,"schema":{"type":"string","title":"Group Id"}}],"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}}},"/api/v1/service-groups/{group_id}/status":{"get":{"summary":"Get Service Group Status","description":"Get aggregated status of a service group.\n\nReturns a summary of the group's overall health and replica statuses.\n\n**Path Parameters:**\n- `group_id`: The service group ID (e.g., \"sg-ba5f6e2462fb\")\n\n**Returns:**\n```json\n{\n  \"group_id\": \"sg-ba5f6e2462fb\",\n  \"overall_status\": \"healthy\",\n  \"total_replicas\": 4,\n  \"healthy_replicas\": 4,\n  \"starting_replicas\": 0,\n  \"pending_replicas\": 0,\n  \"failed_replicas\": 0,\n  \"replica_statuses\": [\n    {\"id\": \"3713894:8001\", \"status\": \"running\"},\n    {\"id\": \"3713894:8002\", \"status\": \"running\"},\n    ...\n  ]\n}\n```\n\n**Overall Status Values:**\n- `healthy`: All replicas are running\n- `partial`: Some replicas are running, others are starting/pending\n- `starting`: All replicas are starting or pending\n- `failed`: All replicas have failed\n- `degraded`: Some replicas have failed, others are running\n\n**Errors:**\n- 404: Service group not found\n\n**Example:**\n- GET `/api/v1/service-groups/sg-ba5f6e2462fb/status`","operationId":"get_service_group_status_api_v1_service_groups__group_id__status_get","parameters":[{"name":"group_id","in":"path","required":true,"schema":{"type":"string","title":"Group Id"}}],"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}}},"/api/v1/services/{service_id}/metrics":{"get":{"summary":"Get Service Metrics","description":"Get Prometheus-compatible metrics from any service (generic endpoint).\n\nThis is a unified metrics endpoint that automatically routes to the appropriate\nservice-specific metrics endpoint based on the service's recipe type.\n\nSupported service types:\n- vLLM inference services (recipe: \"inference/vllm*\")\n- Qdrant vector database (recipe: \"vector-db/qdrant\")\n- Other vector databases (recipe: \"vector-db/*\")\n\n**Path Parameters:**\n- `service_id`: The SLURM job ID of the service\n\n**Returns (Success):**\n- Content-Type: `text/plain; version=0.0.4`\n- Body: Prometheus text format metrics\n\n**Returns (Error):**\n- Content-Type: `application/json`\n- Body: JSON error object with details\n\n**Examples:**\n```bash\n# Get metrics from a service\ncurl http://localhost:8001/api/v1/services/3642874/metrics\n```\n\n**Integration with Prometheus:**\nThis endpoint provides a consistent interface for monitoring, regardless of service type.\nSimply use `/api/v1/services/{service_id}/metrics` for all services.\n\n```yaml\nscrape_configs:\n  - job_name: 'managed-services'\n    static_configs:\n      - targets: ['server:8001']\n    metrics_path: '/api/v1/services/<service_id>/metrics'\n    scrape_interval: 15s\n```\n\n**Note:** This endpoint determines the service type from the recipe_name and routes\nto the appropriate service-specific metrics endpoint (vLLM, Qdrant, etc.).","operationId":"get_service_metrics_api_v1_services__service_id__metrics_get","parameters":[{"name":"service_id","in":"path","required":true,"schema":{"type":"string","title":"Service Id"}}],"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}}},"/api/v1/services/{service_id}/logs":{"get":{"summary":"Get Service Logs","description":"Get SLURM logs (stdout and stderr) from a service.\n\nRetrieves the job output logs written by SLURM. These logs contain:\n- Container build output (if applicable)\n- Application startup messages\n- Runtime logs and errors\n- Container termination information\n\n**Path Parameters:**\n- `service_id`: The SLURM job ID of the service\n\n**Returns:**\n- Object with `logs` field containing the combined stdout/stderr output\n\n**Example Response:**\n```json\n{\n  \"logs\": \"=== SLURM STDOUT ===\\nBuilding Apptainer image...\\nStarting container...\\nApplication startup complete.\"\n}\n```\n\n**Note:** Logs may not be available immediately after job creation. They become available once the job starts running.","operationId":"get_service_logs_api_v1_services__service_id__logs_get","parameters":[{"name":"service_id","in":"path","required":true,"schema":{"type":"string","title":"Service Id"}}],"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}}},"/api/v1/services/{service_id}/status":{"get":{"summary":"Get Service Status","description":"Get the current detailed status of a service.\n\nThis endpoint returns the real-time status by checking both SLURM state and parsing log files\nto determine the exact stage of service initialization.\n\n**Path Parameters:**\n- `service_id`: The SLURM job ID of the service\n\n**Returns:**\n- Object with `status` field containing one of:\n    - `pending`: Job waiting in SLURM queue\n    - `building`: Apptainer container image being built\n    - `starting`: Container launched, application initializing\n    - `running`: Service fully operational\n    - `completed`: Service finished successfully\n    - `failed`: Service encountered an error\n    - `cancelled`: Service was stopped\n    - `unknown`: Unable to determine status\n\n**Example Response:**\n```json\n{\n  \"status\": \"running\"\n}\n```","operationId":"get_service_status_api_v1_services__service_id__status_get","parameters":[{"name":"service_id","in":"path","required":true,"schema":{"type":"string","title":"Service Id"}}],"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}},"post":{"summary":"Update Service Status","description":"Update the status of a service (primarily for cancelling).\n\nThis endpoint allows updating a service's status. Currently supports cancelling services\nby setting status to \"cancelled\", which stops the SLURM job and frees compute resources.\nService metadata and logs are preserved for post-mortem analysis.\n\n**Path Parameters:**\n- `service_id`: The SLURM job ID of the service\n\n**Request Body:**\n- `status`: New status to set. Supported values:\n    - `\"cancelled\"`: Cancel the running SLURM job\n\n**Returns:**\n- Success message with service ID and new status\n\n**Errors:**\n- 400: Invalid status value\n- 404: Service not found or failed to update\n- 500: Error during status update\n\n**Example Request:**\n```bash\ncurl -X POST http://localhost:8001/api/v1/services/3642874/status \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"status\": \"cancelled\"}'\n```\n\n**Example Response:**\n```json\n{\n  \"message\": \"Service 3642874 status updated to cancelled\",\n  \"service_id\": \"3642874\",\n  \"status\": \"cancelled\"\n}\n```\n\n**Note:** This is the recommended way to stop services (instead of DELETE) as it preserves\nservice records for logging and post-mortem analysis, which is essential for Grafana integration.","operationId":"update_service_status_api_v1_services__service_id__status_post","parameters":[{"name":"service_id","in":"path","required":true,"schema":{"type":"string","title":"Service Id"}}],"requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","additionalProperties":{"type":"string"},"examples":{"cancel":{"summary":"Cancel a running service","value":{"status":"cancelled"}}},"title":"Status Update"}}}},"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}}},"/api/v1/recipes":{"get":{"summary":"List Or Get Recipe","description":"List all available recipes OR get a specific recipe by path/name.\n\n**Behavior:**\n- Without query parameters: Returns list of all available recipes\n- With `path` or `name`: Returns a single matching recipe\n\n**Query Parameters (Optional):**\n- `path`: Recipe path (e.g., \"inference/vllm\", \"vector-db/qdrant\")\n- `name`: Recipe display name (e.g., \"vLLM Inference Service\")\n\n**Returns:**\n- If no parameters: Array of all recipe objects\n- If path/name specified: Single recipe object\n\n**Errors:**\n- 404: Recipe not found (when path/name specified)\n\n**Examples:**\n- List all: `GET /api/v1/recipes`\n- Get by path: `GET /api/v1/recipes?path=inference/vllm`\n- Get by path: `GET /api/v1/recipes?path=vector-db/qdrant`\n- Get by name: `GET /api/v1/recipes?name=vLLM%20Inference%20Service`\n\n**Recipe Object Fields:**\n- `name`: Recipe display name\n- `category`: Category (inference, storage, vector-db, etc.)\n- `description`: Human-readable description\n- `version`: Recipe version\n- `path`: Path identifier (e.g., \"inference/vllm\")","operationId":"list_or_get_recipe_api_v1_recipes_get","parameters":[{"name":"path","in":"query","required":false,"schema":{"anyOf":[{"type":"string"},{"type":"null"}],"title":"Path"}},{"name":"name","in":"query","required":false,"schema":{"anyOf":[{"type":"string"},{"type":"null"}],"title":"Name"}}],"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}}},"/api/v1/vllm/services":{"get":{"summary":"List Vllm Services","description":"List all running vLLM inference services with their endpoints.\n\nThis endpoint discovers vLLM services among all running services and resolves their network endpoints.\nUse this to find available vLLM instances for inference requests.\n\n**Returns:**\n- Object with `vllm_services` array, each service containing:\n    - `id`: SLURM job ID (service identifier)\n    - `name`: Service name\n    - `recipe_name`: Recipe used (typically \"inference/vllm\")\n    - `endpoint`: HTTP endpoint URL (e.g., \"http://mel2133:8001\")\n    - `status`: Current status (building/starting/running)\n\n**Example Response:**\n```json\n{\n  \"vllm_services\": [\n    {\n      \"id\": \"3642874\",\n      \"name\": \"vllm-service\",\n      \"recipe_name\": \"inference/vllm\",\n      \"endpoint\": \"http://mel2133:8001\",\n      \"status\": \"running\"\n    }\n  ]\n}\n```\n\n**Status Meanings:**\n- `building`: Container image being built\n- `starting`: vLLM server initializing, not ready for requests\n- `running`: vLLM fully loaded and ready to serve inference requests\n\n**Note:** Only services with status \"running\" are ready to accept prompt requests.","operationId":"list_vllm_services_api_v1_vllm_services_get","responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}}}}},"/api/v1/vllm/available-models":{"get":{"summary":"List Available Vllm Models","description":"Get information about models that can be used with vLLM.\n\nThis endpoint provides information about vLLM's supported model architectures\nand how to find compatible models from HuggingFace Hub. Unlike a hardcoded model list,\nthis returns architectural compatibility information since vLLM can load ANY model\nfrom HuggingFace Hub that uses a supported architecture.\n\n**Key Information:**\n- **Model Source**: All models are downloaded from HuggingFace Hub (https://huggingface.co/models)\n- **Compatibility**: Based on model architecture, not specific model names\n- **How to Use**: Provide any HuggingFace model ID in the `VLLM_MODEL` environment variable\n- **Format**: `organization/model-name` (e.g., `meta-llama/Llama-2-7b-chat-hf`)\n\n**Returns:**\n```json\n{\n  \"model_source\": \"HuggingFace Hub\",\n  \"supported_architectures\": {\n    \"text-generation\": [\"LlamaForCausalLM\", \"MistralForCausalLM\", ...],\n    \"vision-language\": [\"LlavaForConditionalGeneration\", ...],\n    \"embedding\": [\"BertModel\", ...]\n  },\n  \"examples\": {\n    \"GPT-2 (small, for testing)\": \"gpt2\",\n    \"Llama 2 7B Chat\": \"meta-llama/Llama-2-7b-chat-hf\",\n    \"Qwen 2.5 0.5B Instruct\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n    ...\n  },\n  \"how_to_find_models\": [\n    \"Browse HuggingFace: https://huggingface.co/models?pipeline_tag=text-generation\",\n    \"Check model card for architecture\",\n    ...\n  ],\n  \"resource_guidelines\": {\n    \"small_models\": {\n      \"size_range\": \"< 1B parameters\",\n      \"min_gpu_memory_gb\": 4,\n      ...\n    },\n    ...\n  }\n}\n```\n\n**How to Find Compatible Models:**\n1. Browse HuggingFace: https://huggingface.co/models?pipeline_tag=text-generation\n2. Check the model's architecture in its `config.json` file\n3. Verify the architecture is in vLLM's supported list (returned by this endpoint)\n4. Use the model ID when creating a vLLM service\n\n**Example Usage:**\n\nFirst, query this endpoint to see supported architectures and examples:\n```bash\ncurl http://localhost:8001/vllm/available-models\n```\n\nThen create a service with any compatible model:\n```json\n{\n  \"recipe_name\": \"inference/vllm\",\n  \"config\": {\n    \"environment\": {\n      \"VLLM_MODEL\": \"Qwen/Qwen2.5-7B-Instruct\"\n    }\n  }\n}\n```\n\n**Resource Planning:**\nUse the `resource_guidelines` section to estimate GPU memory requirements based on model size.\nLarger models may require multiple GPUs using tensor parallelism.\n\n**Authentication:**\nSome models (e.g., Llama 2, Llama 3) require HuggingFace authentication.\nYou'll need to set up HuggingFace credentials before deploying these models.","operationId":"list_available_vllm_models_api_v1_vllm_available_models_get","responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}}}}},"/api/v1/vllm/search-models":{"get":{"summary":"Search Vllm Models","description":"Search HuggingFace Hub for models compatible with vLLM.\n\nThis endpoint queries the HuggingFace Hub API to find models that match your search criteria\nand checks their compatibility with vLLM's supported architectures.\n\n**Query Parameters:**\n- `query`: Search string (e.g., \"llama\", \"mistral\", \"qwen\", \"instruct\")\n- `architecture`: Filter by specific architecture class name\n- `limit`: Maximum results to return (1-100, default: 20)\n- `sort_by`: Sort order - \"downloads\", \"likes\", \"trending\", or \"created_at\"\n\n**Returns:**\n```json\n{\n  \"models\": [\n    {\n      \"id\": \"meta-llama/Llama-2-7b-chat-hf\",\n      \"downloads\": 1500000,\n      \"likes\": 5000,\n      \"architecture\": \"LlamaForCausalLM\",\n      \"vllm_compatible\": true,\n      \"created_at\": \"2023-07-18T...\",\n      \"tags\": [\"llama\", \"text-generation\", \"conversational\"]\n    },\n    ...\n  ],\n  \"total\": 20\n}\n```\n\n**Example Searches:**\n\nFind popular Llama models:\n```\nGET /vllm/search-models?query=llama&sort_by=downloads&limit=10\n```\n\nFind Qwen instruction models:\n```\nGET /vllm/search-models?query=qwen+instruct&limit=15\n```\n\nFind all models with specific architecture:\n```\nGET /vllm/search-models?architecture=MistralForCausalLM\n```\n\n**Use Case:**\nUse this to discover new models before creating a vLLM service. The `vllm_compatible`\nfield indicates whether the model uses an architecture supported by vLLM.","operationId":"search_vllm_models_api_v1_vllm_search_models_get","parameters":[{"name":"query","in":"query","required":false,"schema":{"anyOf":[{"type":"string"},{"type":"null"}],"description":"Search query (e.g., 'llama', 'mistral', 'qwen')","title":"Query"},"description":"Search query (e.g., 'llama', 'mistral', 'qwen')"},{"name":"architecture","in":"query","required":false,"schema":{"anyOf":[{"type":"string"},{"type":"null"}],"description":"Filter by architecture (e.g., 'LlamaForCausalLM')","title":"Architecture"},"description":"Filter by architecture (e.g., 'LlamaForCausalLM')"},{"name":"limit","in":"query","required":false,"schema":{"type":"integer","maximum":100,"minimum":1,"description":"Maximum number of results (1-100)","default":20,"title":"Limit"},"description":"Maximum number of results (1-100)"},{"name":"sort_by","in":"query","required":false,"schema":{"type":"string","description":"Sort by: downloads, likes, trending, created_at","default":"downloads","title":"Sort By"},"description":"Sort by: downloads, likes, trending, created_at"}],"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}}},"/api/v1/vllm/model-info/{model_id}":{"get":{"summary":"Get Model Info","description":"Get detailed information about a specific model from HuggingFace Hub.\n\nThis endpoint fetches comprehensive information about a model including its architecture,\nsize, compatibility with vLLM, and download statistics.\n\n**Path Parameters:**\n- `model_id`: HuggingFace model ID (e.g., \"meta-llama/Llama-2-7b-hf\", \"Qwen/Qwen2.5-3B-Instruct\")\n\n**Returns:**\n```json\n{\n  \"id\": \"Qwen/Qwen2.5-3B-Instruct\",\n  \"architecture\": \"Qwen2ForCausalLM\",\n  \"vllm_compatible\": true,\n  \"task_type\": \"text-generation\",\n  \"downloads\": 250000,\n  \"likes\": 1200,\n  \"tags\": [\"qwen2\", \"instruct\", \"chat\"],\n  \"size_bytes\": 6442450944,\n  \"size_gb\": 6.0,\n  \"pipeline_tag\": \"text-generation\",\n  \"library_name\": \"transformers\"\n}\n```\n\n**Fields:**\n- `vllm_compatible`: Whether this model can be loaded by vLLM\n- `task_type`: Type of task (text-generation, vision-language, embedding)\n- `size_gb`: Approximate model size in gigabytes\n- `architecture`: The model's architecture class\n\n**Example Usage:**\n\nCheck if a model is compatible before deployment:\n```bash\ncurl http://localhost:8001/vllm/model-info/Qwen/Qwen2.5-7B-Instruct\n```\n\nThen use the model ID to create a service:\n```json\n{\n  \"recipe_name\": \"inference/vllm\",\n  \"config\": {\n    \"environment\": {\n      \"VLLM_MODEL\": \"Qwen/Qwen2.5-7B-Instruct\"\n    }\n  }\n}\n```\n\n**Note:** Some models require HuggingFace authentication. Check the model page on\nHuggingFace Hub if you encounter access errors.","operationId":"get_model_info_api_v1_vllm_model_info__model_id__get","parameters":[{"name":"model_id","in":"path","required":true,"schema":{"type":"string","title":"Model Id"}}],"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}}},"/api/v1/vector-db/services":{"get":{"summary":"List Vector Db Services","description":"List all running vector database services.\n\nReturns a list of vector database services (Qdrant, Chroma, etc.) with their endpoints.\n\n**Example Response:**\n```json\n{\n  \"vector_db_services\": [\n    {\n      \"id\": \"3642875\",\n      \"name\": \"qdrant-service\",\n      \"recipe_name\": \"vector-db/qdrant\",\n      \"endpoint\": \"http://mel2079:6333\",\n      \"status\": \"running\"\n    }\n  ]\n}\n```","operationId":"list_vector_db_services_api_v1_vector_db_services_get","responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}}}}},"/api/v1/vector-db/{service_id}/collections":{"get":{"summary":"Get Collections","description":"Get list of collections from a vector database service.\n\n**Path Parameters:**\n- `service_id`: SLURM job ID of the vector DB service\n\n**Returns (Success):**\n```json\n{\n  \"success\": true,\n  \"collections\": [\"my_documents\", \"embeddings\"],\n  \"service_id\": \"3642875\",\n  \"endpoint\": \"http://mel2079:6333\"\n}\n```\n\n**Returns (Service Not Ready):**\n```json\n{\n  \"success\": false,\n  \"error\": \"Service is not ready yet (status: starting)\",\n  \"message\": \"The vector DB service is still starting up. Please wait.\",\n  \"collections\": []\n}\n```","operationId":"get_collections_api_v1_vector_db__service_id__collections_get","parameters":[{"name":"service_id","in":"path","required":true,"schema":{"type":"string","title":"Service Id"}}],"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}}},"/api/v1/vector-db/{service_id}/collections/{collection_name}":{"get":{"summary":"Get Collection Info","description":"Get detailed information about a specific collection.\n\n**Path Parameters:**\n- `service_id`: SLURM job ID of the vector DB service\n- `collection_name`: Name of the collection\n\n**Returns (Success):**\n```json\n{\n  \"success\": true,\n  \"collection_info\": {\n    \"status\": \"green\",\n    \"vectors_count\": 1000,\n    \"indexed_vectors_count\": 1000,\n    \"config\": {\n      \"params\": {\n        \"vectors\": {\n          \"size\": 384,\n          \"distance\": \"Cosine\"\n        }\n      }\n    }\n  },\n  \"service_id\": \"3642875\",\n  \"collection_name\": \"my_documents\"\n}\n```","operationId":"get_collection_info_api_v1_vector_db__service_id__collections__collection_name__get","parameters":[{"name":"service_id","in":"path","required":true,"schema":{"type":"string","title":"Service Id"}},{"name":"collection_name","in":"path","required":true,"schema":{"type":"string","title":"Collection Name"}}],"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}},"put":{"summary":"Create Collection","description":"Create a new collection in the vector database.\n\n**Path Parameters:**\n- `service_id`: SLURM job ID of the vector DB service\n- `collection_name`: Name for the new collection\n\n**Request Body:**\n- `vector_size` (required): Dimension of vectors (e.g., 384, 768, 1536)\n- `distance` (optional): Distance metric - \"Cosine\" (default), \"Euclid\", or \"Dot\"\n\n**Returns (Success):**\n```json\n{\n  \"success\": true,\n  \"message\": \"Collection 'my_documents' created successfully\",\n  \"collection_name\": \"my_documents\",\n  \"vector_size\": 384,\n  \"distance\": \"Cosine\"\n}\n```\n\n**Example:**\n```bash\ncurl -X PUT \"http://localhost:8001/api/v1/vector-db/3642875/collections/my_docs\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"vector_size\": 384, \"distance\": \"Cosine\"}'\n```","operationId":"create_collection_api_v1_vector_db__service_id__collections__collection_name__put","parameters":[{"name":"service_id","in":"path","required":true,"schema":{"type":"string","title":"Service Id"}},{"name":"collection_name","in":"path","required":true,"schema":{"type":"string","title":"Collection Name"}}],"requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","examples":{"basic":{"summary":"Create a basic collection","value":{"vector_size":384,"distance":"Cosine"}},"euclidean":{"summary":"Create collection with Euclidean distance","value":{"vector_size":768,"distance":"Euclid"}}},"title":"Request"}}}},"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}},"delete":{"summary":"Delete Collection","description":"Delete a collection from the vector database.\n\n**Path Parameters:**\n- `service_id`: SLURM job ID of the vector DB service\n- `collection_name`: Name of the collection to delete\n\n**Returns (Success):**\n```json\n{\n  \"success\": true,\n  \"message\": \"Collection 'my_documents' deleted successfully\",\n  \"collection_name\": \"my_documents\"\n}\n```\n\n**Example:**\n```bash\ncurl -X DELETE \"http://localhost:8001/api/v1/vector-db/3642875/collections/my_docs\"\n```","operationId":"delete_collection_api_v1_vector_db__service_id__collections__collection_name__delete","parameters":[{"name":"service_id","in":"path","required":true,"schema":{"type":"string","title":"Service Id"}},{"name":"collection_name","in":"path","required":true,"schema":{"type":"string","title":"Collection Name"}}],"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}}},"/api/v1/vector-db/{service_id}/collections/{collection_name}/points":{"put":{"summary":"Upsert Points","description":"Insert or update points (vectors with payloads) in a collection.\n\n**Path Parameters:**\n- `service_id`: SLURM job ID of the vector DB service\n- `collection_name`: Name of the collection\n\n**Request Body:**\n- `points` (required): List of points to upsert\n  - Each point must have: `id`, `vector`, and optional `payload`\n\n**Returns (Success):**\n```json\n{\n  \"success\": true,\n  \"message\": \"Upserted 2 points to collection 'my_documents'\",\n  \"num_points\": 2\n}\n```\n\n**Example:**\n```bash\ncurl -X PUT \"http://localhost:8001/api/v1/vector-db/3642875/collections/my_docs/points\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"points\": [\n      {\"id\": 1, \"vector\": [0.1, 0.2, 0.3], \"payload\": {\"text\": \"Hello world\"}}\n    ]\n  }'\n```","operationId":"upsert_points_api_v1_vector_db__service_id__collections__collection_name__points_put","parameters":[{"name":"service_id","in":"path","required":true,"schema":{"type":"string","title":"Service Id"}},{"name":"collection_name","in":"path","required":true,"schema":{"type":"string","title":"Collection Name"}}],"requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","examples":{"simple":{"summary":"Insert a single point","value":{"points":[{"id":1,"vector":[0.1,0.2,0.3,0.4],"payload":{"text":"Example document"}}]}},"multiple":{"summary":"Insert multiple points","value":{"points":[{"id":1,"vector":[0.1,0.2,0.3],"payload":{"text":"First doc"}},{"id":2,"vector":[0.4,0.5,0.6],"payload":{"text":"Second doc"}}]}}},"title":"Request"}}}},"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}}},"/api/v1/vector-db/{service_id}/collections/{collection_name}/points/search":{"post":{"summary":"Search Points","description":"Search for similar vectors in a collection.\n\n**Path Parameters:**\n- `service_id`: SLURM job ID of the vector DB service\n- `collection_name`: Name of the collection to search\n\n**Request Body:**\n- `query_vector` (required): The query vector to find similar items\n- `limit` (optional): Maximum number of results (default: 10)\n\n**Returns (Success):**\n```json\n{\n  \"success\": true,\n  \"results\": [\n    {\n      \"id\": 1,\n      \"score\": 0.95,\n      \"payload\": {\"text\": \"Similar document\"}\n    }\n  ],\n  \"num_results\": 1\n}\n```\n\n**Example:**\n```bash\ncurl -X POST \"http://localhost:8001/api/v1/vector-db/3642875/collections/my_docs/points/search\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"query_vector\": [0.1, 0.2, 0.3],\n    \"limit\": 5\n  }'\n```","operationId":"search_points_api_v1_vector_db__service_id__collections__collection_name__points_search_post","parameters":[{"name":"service_id","in":"path","required":true,"schema":{"type":"string","title":"Service Id"}},{"name":"collection_name","in":"path","required":true,"schema":{"type":"string","title":"Collection Name"}}],"requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","examples":{"basic":{"summary":"Basic similarity search","value":{"query_vector":[0.1,0.2,0.3,0.4],"limit":5}}},"title":"Request"}}}},"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}}},"/api/v1/vllm/{service_id}/prompt":{"post":{"summary":"Send a prompt to a running vLLM service","description":"Send a text prompt to a running vLLM inference service and get a response.\n\nThis endpoint forwards your prompt to the vLLM service using the OpenAI-compatible API.\nThe vLLM service must be in \"running\" status (not \"building\" or \"starting\").\n\n**Path Parameters:**\n- `service_id`: SLURM job ID of the vLLM service (from list_vllm_services)\n\n**Request Body:**\n- `prompt` (required): Text prompt to send to the model\n- `model` (optional): Model identifier (auto-discovered if omitted)\n- `max_tokens` (optional): Maximum tokens to generate (default: 150)\n- `temperature` (optional): Sampling temperature 0.0-2.0 (default: 0.7)\n\n**Returns (Success):**\n```json\n{\n  \"success\": true,\n  \"response\": \"Generated text response from the model...\",\n  \"service_id\": \"3642874\",\n  \"endpoint\": \"http://mel2133:8001\",\n  \"usage\": {\"prompt_tokens\": 10, \"completion_tokens\": 25, \"total_tokens\": 35}\n}\n```\n\n**Returns (Error):**\n```json\n{\n  \"success\": false,\n  \"error\": \"Failed to connect to VLLM service: Connection refused\",\n  \"endpoint\": \"http://mel2133:8001\"\n}\n```\n\n**Errors:**\n- 400: Prompt is missing or invalid\n- 404: vLLM service not found\n- 500: Service error or connection failure\n\n**Example:**\n```bash\ncurl -X POST \"http://server:8000/api/v1/vllm/3642874/prompt\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"prompt\": \"What is AI?\", \"max_tokens\": 100}'\n```\n\n**Note:** The vLLM service must be fully initialized (status=\"running\") before it can accept prompts.","operationId":"prompt_vllm_service_api_v1_vllm__service_id__prompt_post","parameters":[{"name":"service_id","in":"path","required":true,"schema":{"type":"string","title":"Service Id"}}],"requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","examples":{"simple":{"summary":"Basic prompt","value":{"prompt":"Write a short haiku about AI."}},"with_model":{"summary":"Prompt specifying model","value":{"prompt":"Hello","model":"gpt2","max_tokens":64}}},"title":"Request"}}}},"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}}},"/api/v1/vllm/{service_id}/models":{"get":{"summary":"Get Vllm Models","description":"Get the list of models served by a running vLLM service.\n\nQueries the vLLM service's /v1/models endpoint to discover which models are loaded and available.\nThis is useful when you don't know which model to specify in prompt requests.\n\n**Path Parameters:**\n- `service_id`: SLURM job ID of the vLLM service\n\n**Returns (Success):**\n```json\n{\n  \"success\": true,\n  \"models\": [\"Qwen/Qwen3-0.6B\", \"gpt2\"],\n  \"service_id\": \"3642874\",\n  \"endpoint\": \"http://mel2079:8000\"\n}\n```\n\n**Returns (Service Not Ready):**\n```json\n{\n  \"success\": false,\n  \"error\": \"Service is not ready yet (status: starting)\",\n  \"message\": \"The vLLM service is still starting up (status: starting). Please wait a moment and try again.\",\n  \"service_id\": \"3642874\",\n  \"status\": \"starting\",\n  \"models\": []\n}\n```\n\n**Returns (No Models):**\n```json\n{\n  \"success\": true,\n  \"models\": [],\n  \"service_id\": \"3642874\",\n  \"endpoint\": \"http://mel2079:8000\"\n}\n```\n\n**Example:**\n- GET `/api/v1/vllm/3642874/models`\n\n**Note:** \n- The response now includes success status and detailed error messages\n- Check the `success` field to determine if the operation succeeded\n- Model names can be used in the `model` parameter of the prompt endpoint","operationId":"get_vllm_models_api_v1_vllm__service_id__models_get","parameters":[{"name":"service_id","in":"path","required":true,"schema":{"type":"string","title":"Service Id"}}],"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}}},"/api/v1/vllm/{service_id}/metrics":{"get":{"summary":"Get Vllm Metrics","description":"Get Prometheus-compatible metrics from a running vLLM service.\n\nvLLM natively exposes comprehensive metrics on the /metrics endpoint in Prometheus text format.\nThese metrics can be scraped by Prometheus, Grafana, or any other monitoring system that supports\nthe Prometheus metrics format.\n\n**Available Metrics Include:**\n- `vllm_request_count`: Total number of requests processed\n- `vllm_request_duration_seconds`: Request latency distribution (histogram)\n- `vllm_tokens_generated`: Total tokens generated by the model\n- `vllm_cache_usage_ratio`: KV cache usage as a ratio (0.0-1.0)\n- `vllm_scheduling_delays`: Scheduling delays in the batch processor\n- `vllm_running_requests`: Number of requests currently being processed\n- Plus many more detailed metrics for performance monitoring\n\n**Path Parameters:**\n- `service_id`: SLURM job ID of the vLLM service\n\n**Available Metrics (vLLM-specific):**\n- `vllm:num_requests_running` - Number of requests currently being processed\n- `vllm:num_requests_waiting` - Number of requests waiting in queue\n- `vllm:kv_cache_usage_perc` - KV cache utilization percentage (0.0-1.0)\n- `vllm:prompt_tokens_total` - Total number of prompt tokens processed\n- `vllm:generation_tokens_total` - Total number of tokens generated\n- `vllm:time_to_first_token_seconds` - Histogram of time to first token\n- `vllm:time_per_output_token_seconds` - Histogram of time per output token\n- `vllm:e2e_request_latency_seconds` - End-to-end request latency histogram\n\n**Returns (Success):**\n- Content-Type: `text/plain; version=0.0.4`\n- Body: Prometheus text format metrics\n\n**Example Success Response:**\n```\n# HELP vllm:num_requests_running Number of requests currently running\n# TYPE vllm:num_requests_running gauge\nvllm:num_requests_running{engine=\"0\",model_name=\"gpt2\"} 0.0\n# HELP vllm:prompt_tokens_total Total prompt tokens processed\n# TYPE vllm:prompt_tokens_total counter\nvllm:prompt_tokens_total{engine=\"0\",model_name=\"gpt2\"} 150.0\n...\n```\n\n**Returns (Error):**\n- Content-Type: `application/json`\n- Body: JSON error object\n\n**Example Error Response:**\n```json\n{\n  \"success\": false,\n  \"error\": \"Service is not ready yet (status: starting)\",\n  \"message\": \"The vLLM service is still starting up. Please wait.\",\n  \"service_id\": \"3642874\",\n  \"status\": \"starting\"\n}\n```\n\n**Integration with Prometheus:**\nAdd this endpoint to your Prometheus scrape configuration:\n```yaml\nscrape_configs:\n  - job_name: 'vllm-services'\n    static_configs:\n      - targets: ['server:8001']\n    metrics_path: '/api/v1/vllm/<service_id>/metrics'\n    scrape_interval: 15s\n```\n\n**Example:**\n```bash\ncurl http://localhost:8001/api/v1/vllm/3642874/metrics\n```\n\n**Note:**\n- Metrics are returned in Prometheus text format (not JSON)\n- This endpoint is lightweight and safe to call frequently (every 5-15 seconds)\n- Metrics are cumulative counters and gauges - use Prometheus functions like `rate()` for analysis","operationId":"get_vllm_metrics_api_v1_vllm__service_id__metrics_get","parameters":[{"name":"service_id","in":"path","required":true,"schema":{"type":"string","title":"Service Id"}}],"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}}},"/api/v1/vector-db/{service_id}/metrics":{"get":{"summary":"Get Qdrant Metrics","description":"Get Prometheus metrics from a running Qdrant vector database service.\n\nQueries the Qdrant service's /metrics endpoint to retrieve Prometheus-compatible metrics.\nQdrant natively exposes metrics about collections, points, search operations, and performance.\n\n**Path Parameters:**\n- `service_id`: SLURM job ID of the Qdrant service\n\n**Available Metrics (Qdrant-specific):**\n- `app_info` - Application information (version, features)\n- `app_status_recovery_mode` - Whether recovery mode is active\n- `collections_total` - Total number of collections\n- `collections_vector_total` - Total number of vectors across all collections\n- `collections_full_total` - Number of fully optimized collections\n- `rest_responses_total` - Total REST API responses by status code\n- `rest_responses_duration_seconds` - REST API response time histogram\n- `grpc_responses_total` - Total gRPC responses by status code\n- `grpc_responses_duration_seconds` - gRPC response time histogram\n\n**Returns (Success):**\n- Content-Type: `text/plain; version=0.0.4`\n- Body: Prometheus text format metrics\n\n**Example Success Response:**\n```\n# HELP app_info information about qdrant server\n# TYPE app_info gauge\napp_info{name=\"qdrant\",version=\"1.7.0\"} 1\n# HELP collections_total Number of collections\n# TYPE collections_total gauge\ncollections_total 3\n# HELP rest_responses_total Total number of responses\n# TYPE rest_responses_total counter\nrest_responses_total{status=\"200\"} 45\n...\n```\n\n**Returns (Error):**\n- Content-Type: `application/json`\n- Body: JSON error object\n\n**Example Error Response:**\n```json\n{\n  \"success\": false,\n  \"error\": \"Service is not ready yet (status: starting)\",\n  \"message\": \"The Qdrant service is still starting up. Please wait.\",\n  \"service_id\": \"3642875\",\n  \"status\": \"starting\"\n}\n```\n\n**Integration with Prometheus:**\nAdd this endpoint to your Prometheus scrape configuration:\n```yaml\nscrape_configs:\n  - job_name: 'qdrant-services'\n    static_configs:\n      - targets: ['server:8001']\n    metrics_path: '/api/v1/vector-db/<service_id>/metrics'\n    scrape_interval: 15s\n```\n\n**Example:**\n```bash\ncurl http://localhost:8001/api/v1/vector-db/3642875/metrics\n```\n\n**Note:**\n- Metrics are returned in Prometheus text format (not JSON)\n- This endpoint is lightweight and safe to call frequently (every 5-15 seconds)\n- Metrics provide insights into collection sizes, API usage, and performance","operationId":"get_qdrant_metrics_api_v1_vector_db__service_id__metrics_get","parameters":[{"name":"service_id","in":"path","required":true,"schema":{"type":"string","title":"Service Id"}}],"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}}}},"components":{"schemas":{"HTTPValidationError":{"properties":{"detail":{"items":{"$ref":"#/components/schemas/ValidationError"},"type":"array","title":"Detail"}},"type":"object","title":"HTTPValidationError"},"ServiceRequest":{"properties":{"recipe_name":{"type":"string","title":"Recipe Name"},"config":{"type":"object","title":"Config","default":{}}},"type":"object","required":["recipe_name"],"title":"ServiceRequest","description":"Schema for service creation requests."},"ServiceResponse":{"properties":{"id":{"type":"string","title":"Id"},"name":{"type":"string","title":"Name"},"recipe_name":{"type":"string","title":"Recipe Name"},"status":{"type":"string","title":"Status"},"config":{"type":"object","title":"Config"},"created_at":{"type":"string","title":"Created At"},"type":{"anyOf":[{"type":"string"},{"type":"null"}],"title":"Type"},"replicas":{"anyOf":[{"items":{"type":"object"},"type":"array"},{"type":"null"}],"title":"Replicas"},"num_replicas":{"anyOf":[{"type":"integer"},{"type":"null"}],"title":"Num Replicas"},"num_nodes":{"anyOf":[{"type":"integer"},{"type":"null"}],"title":"Num Nodes"},"replicas_per_node":{"anyOf":[{"type":"integer"},{"type":"null"}],"title":"Replicas Per Node"},"total_replicas":{"anyOf":[{"type":"integer"},{"type":"null"}],"title":"Total Replicas"},"node_jobs":{"anyOf":[{"items":{"type":"object"},"type":"array"},{"type":"null"}],"title":"Node Jobs"},"group_id":{"anyOf":[{"type":"string"},{"type":"null"}],"title":"Group Id"},"replica_index":{"anyOf":[{"type":"integer"},{"type":"null"}],"title":"Replica Index"}},"type":"object","required":["id","name","recipe_name","status","config","created_at"],"title":"ServiceResponse","description":"Schema for service responses.\n\nFor regular services, this contains basic service information.\nFor service groups (replicas), includes additional fields for group management."},"ValidationError":{"properties":{"loc":{"items":{"anyOf":[{"type":"string"},{"type":"integer"}]},"type":"array","title":"Location"},"msg":{"type":"string","title":"Message"},"type":{"type":"string","title":"Error Type"}},"type":"object","required":["loc","msg","type"],"title":"ValidationError"}}}}