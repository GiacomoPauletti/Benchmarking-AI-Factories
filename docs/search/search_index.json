{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"AI Factory Benchmarking Framework","text":"<p>Welcome to the AI Factory Benchmarking Framework documentation.</p>"},{"location":"#overview","title":"Overview","text":"<p>This framework enables benchmarking of AI Factory components on HPC systems, specifically designed for the MeluXina supercomputer. The application orchestrates AI workloads via SLURM and provides monitoring in real-time, all controlable through a Grafana UI.</p>"},{"location":"#architecture","title":"Architecture","text":"<p>The framework consists of multiple microservices working together:</p> <pre><code>graph TB\n    subgraph Frontend[\"Frontend (localhost)\"]\n        Grafana[Grafana&lt;br/&gt;:3000]\n    end\n\n    subgraph APIs[\"API Services\"]\n        Server[Server&lt;br/&gt;:8001]\n        Client[Client&lt;br/&gt;:8002]\n        Logs[Logs&lt;br/&gt;:8004]\n        Monitoring[Monitoring&lt;br/&gt;:8005]\n    end\n\n    subgraph Metrics[\"Metrics Stack\"]\n        Prometheus[Prometheus&lt;br/&gt;:9090]\n        Pushgateway[Pushgateway&lt;br/&gt;:9091]\n        Loki[Loki&lt;br/&gt;:3100]\n        Alloy[Alloy]\n    end\n\n    subgraph HPC[\"MeluXina HPC\"]\n        Orchestrator[ServiceOrchestrator]\n        SLURM[SLURM]\n        Compute[Compute Nodes&lt;br/&gt;vLLM / GPU Exporters]\n    end\n\n    Grafana --&gt; Prometheus\n    Grafana --&gt; Loki\n    Server --&gt;|SSH Tunnel| Orchestrator\n    Client --&gt;|SSH + SLURM| SLURM\n    Orchestrator --&gt; SLURM\n    SLURM --&gt; Compute\n    Compute --&gt;|Push Metrics| Pushgateway\n    Prometheus --&gt; Pushgateway\n    Alloy --&gt; Loki\n    Logs --&gt;|rsync| HPC\n\n    style Grafana fill:#FFE0B2\n    style Server fill:#B3E5FC\n    style Prometheus fill:#C8E6C9</code></pre>"},{"location":"#services","title":"Services","text":"Service Port Description Server 8001 Core orchestration - manages SLURM jobs and AI workload deployment Client 8002 Executes distributed load tests against AI services Logs 8004 Syncs and categorizes SLURM job logs from MeluXina Monitoring 8005 Manages Prometheus scrape targets and metrics collection Grafana 3000 Visualization dashboard for metrics and benchmarks Prometheus 9090 Time-series metrics storage Pushgateway 9091 Buffers metrics from HPC compute nodes"},{"location":"#quick-start","title":"Quick Start","text":"<p>See Getting Started for detailed instructions.</p>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Getting Started - Setup and installation</li> <li>Architecture - System design and components</li> <li>Server Service - Detailed server documentation</li> <li>API Reference - Interactive API documentation</li> <li>Development - Development guidelines</li> </ul>"},{"location":"#status","title":"Status","text":"<p>Current Version: 1.0.0 Last Updated: January 2026 Project: EUMaster4HPC Challenge 2025-2026</p>"},{"location":"api/client/","title":"Client API Documentation","text":""},{"location":"api/client/#overview","title":"Overview","text":"<p>The Client API (port <code>8002</code>) manages distributed load testing against AI inference services. It creates client groups on HPC compute nodes that generate concurrent workloads.</p>"},{"location":"api/client/#interactive-api-reference","title":"Interactive API Reference","text":""},{"location":"api/client/#quick-start","title":"Quick Start","text":""},{"location":"api/client/#create-a-client-group","title":"Create a Client Group","text":"<pre><code>curl -X POST http://localhost:8002/api/v1/client-groups \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"service_id\": \"3652098\",\n    \"num_clients\": 10,\n    \"requests_per_second\": 2.0,\n    \"duration_seconds\": 60,\n    \"prompts\": [\"Write a poem about AI\", \"Explain machine learning\"],\n    \"max_tokens\": 100,\n    \"time_limit\": 10\n  }'\n</code></pre>"},{"location":"api/client/#list-client-groups","title":"List Client Groups","text":"<pre><code>curl http://localhost:8002/api/v1/client-groups\n</code></pre>"},{"location":"api/client/#get-client-group-status","title":"Get Client Group Status","text":"<pre><code>curl http://localhost:8002/api/v1/client-groups/{group_id}\n</code></pre>"},{"location":"api/client/#stop-a-client-group","title":"Stop a Client Group","text":"<pre><code>curl -X DELETE http://localhost:8002/api/v1/client-groups/{group_id}\n</code></pre>"},{"location":"api/client/#see-also","title":"See Also","text":"<ul> <li>Client Service Overview - Architecture and concepts</li> <li>Server API - Create vLLM services to benchmark</li> </ul>"},{"location":"api/logs/","title":"Logs API Documentation","text":""},{"location":"api/logs/#overview","title":"Overview","text":"<p>The Logs API (port <code>8004</code>) provides access to SLURM job logs synced from MeluXina, automatically categorized by service type.</p>"},{"location":"api/logs/#interactive-api-reference","title":"Interactive API Reference","text":""},{"location":"api/logs/#quick-start","title":"Quick Start","text":""},{"location":"api/logs/#check-sync-status","title":"Check Sync Status","text":"<pre><code>curl http://localhost:8004/status\n</code></pre>"},{"location":"api/logs/#list-service-categories","title":"List Service Categories","text":"<pre><code>curl http://localhost:8004/services\n</code></pre> <p>Returns: <code>[\"server\", \"client\", \"vllm\", \"vector-db\", \"monitoring\", \"uncategorized\"]</code></p>"},{"location":"api/logs/#query-logs","title":"Query Logs","text":"<pre><code># All logs (paginated)\ncurl \"http://localhost:8004/logs?limit=50\"\n\n# Filter by service\ncurl \"http://localhost:8004/logs?service=client&amp;limit=20\"\n</code></pre>"},{"location":"api/logs/#get-log-content","title":"Get Log Content","text":"<pre><code># Full content\ncurl \"http://localhost:8004/logs/content?path=categorized/client/loadgen-12345.out\"\n\n# Last 100 lines\ncurl \"http://localhost:8004/logs/content?path=categorized/client/loadgen-12345.out&amp;tail=100\"\n</code></pre>"},{"location":"api/logs/#trigger-manual-sync","title":"Trigger Manual Sync","text":"<pre><code>curl -X POST http://localhost:8004/sync/trigger\n</code></pre>"},{"location":"api/logs/#get-service-statistics","title":"Get Service Statistics","text":"<pre><code>curl http://localhost:8004/services/stats\n</code></pre>"},{"location":"api/logs/#log-categories","title":"Log Categories","text":"<p>Logs are automatically categorized by filename patterns:</p> Category Patterns <code>client</code> <code>loadgen-*.out</code>, <code>loadgen-*.err</code> <code>vllm</code> <code>vllm-*.out</code>, <code>inference-*.out</code> <code>vector-db</code> <code>vectordb-*.out</code>, <code>qdrant-*.out</code> <code>server</code> <code>server-*.out</code>, <code>orchestrator-*.out</code> <code>monitoring</code> <code>prometheus-*.out</code>, <code>exporter-*.out</code>"},{"location":"api/logs/#see-also","title":"See Also","text":"<ul> <li>Logs Service Overview - Architecture and configuration</li> <li>Server API - Create services that generate logs</li> </ul>"},{"location":"api/monitoring/","title":"Monitoring API Documentation","text":""},{"location":"api/monitoring/#overview","title":"Overview","text":"<p>The Monitoring API (port <code>8005</code>) manages Prometheus-based metrics collection for benchmarking sessions.</p>"},{"location":"api/monitoring/#interactive-api-reference","title":"Interactive API Reference","text":""},{"location":"api/monitoring/#quick-start","title":"Quick Start","text":""},{"location":"api/monitoring/#create-a-monitoring-session","title":"Create a Monitoring Session","text":"<pre><code>curl -X POST http://localhost:8005/api/v1/sessions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"run_id\": \"benchmark-01\",\n    \"prometheus_port\": 9090,\n    \"prom_host\": \"localhost\"\n  }'\n</code></pre>"},{"location":"api/monitoring/#start-prometheus","title":"Start Prometheus","text":"<pre><code>curl -X POST http://localhost:8005/api/v1/sessions/benchmark-01/start \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"partition\": \"cpu\",\n    \"time_limit\": \"02:00:00\"\n  }'\n</code></pre>"},{"location":"api/monitoring/#register-a-service-target","title":"Register a Service Target","text":"<pre><code>curl -X POST http://localhost:8005/api/v1/services \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"session_id\": \"benchmark-01\",\n    \"client_id\": \"client-001\",\n    \"name\": \"vllm\",\n    \"endpoint\": \"http://mel2153:8000/metrics\"\n  }'\n</code></pre>"},{"location":"api/monitoring/#collect-metrics","title":"Collect Metrics","text":"<pre><code>curl -X POST http://localhost:8005/api/v1/sessions/benchmark-01/collect \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"window_start\": \"2026-01-08T10:00:00Z\",\n    \"window_end\": \"2026-01-08T11:00:00Z\",\n    \"out_dir\": \"/app/logs/metrics\",\n    \"run_id\": \"run-001\"\n  }'\n</code></pre>"},{"location":"api/monitoring/#stop-session","title":"Stop Session","text":"<pre><code>curl -X POST http://localhost:8005/api/v1/sessions/benchmark-01/stop\n</code></pre>"},{"location":"api/monitoring/#see-also","title":"See Also","text":"<ul> <li>Monitoring Service Overview - Architecture and GPU metrics</li> <li>Architecture - System design</li> </ul>"},{"location":"api/server/","title":"Server API Documentation","text":""},{"location":"api/server/#architecture-overview","title":"Architecture Overview","text":"<p>The system uses a gateway-orchestrator architecture with two components:</p> <pre><code>graph LR\n    Client[\"Client&lt;br/&gt;(Your Code)\"] --&gt;|\"HTTP&lt;br/&gt;localhost:8001\"| Server[\"Server&lt;br/&gt;(Gateway)\"]\n    Server --&gt;|\"SSH Tunnel\"| Orch[\"ServiceOrchestrator&lt;br/&gt;(MeluXina)\"]\n    Orch --&gt;|\"SLURM\"| Compute[\"Compute Nodes\"]\n\n    InternalClient[\"Client on&lt;br/&gt;MeluXina\"] -.-&gt;|\"HTTP&lt;br/&gt;orchestrator:8000&lt;br/&gt;(Direct)\"| Orch\n\n    style Server fill:#B3E5FC,stroke:#0288D1,stroke-width:2px\n    style Orch fill:#C8E6C9,stroke:#388E3C,stroke-width:2px\n    style InternalClient stroke-dasharray: 5 5</code></pre> <p>Two Access Patterns:</p> <ol> <li>External Access (typical): <code>Client \u2192 Server (localhost:8001) \u2192 SSH \u2192 Orchestrator \u2192 SLURM</code></li> <li>Use when running client code on your laptop/workstation</li> <li>Server acts as a gateway, forwarding all requests to MeluXina</li> <li> <p>All examples below use this pattern</p> </li> <li> <p>Internal Access (advanced): <code>Client \u2192 Orchestrator (orchestrator:8000) directly</code></p> </li> <li>Use when running client code on MeluXina (e.g., in a compute job)</li> <li>Bypasses the gateway for lower latency</li> <li>Requires access to MeluXina internal network</li> </ol> <p>API Endpoint Translation:</p> Client Request Server Gateway Orchestrator Internal API <code>POST /api/v1/services</code> Proxies to \u2192 <code>POST /api/services/start</code> <code>GET /api/v1/services</code> Proxies to \u2192 <code>GET /api/services</code> <code>DELETE /api/v1/services/{id}</code> Proxies to \u2192 <code>POST /api/services/stop/{id}</code> <code>POST /api/v1/services/{id}/status</code> Proxies to \u2192 Handled internally by orchestrator <code>POST /api/v1/vllm/{id}/prompt</code> Proxies to \u2192 <code>POST /api/services/vllm/{id}/prompt</code>"},{"location":"api/server/#interactive-api-reference","title":"Interactive API Reference","text":"<p>The Server Gateway provides a REST API for managing SLURM jobs and AI workload orchestration.</p> <p></p>"},{"location":"api/server/#api-examples","title":"API Examples","text":""},{"location":"api/server/#create-vllm-service-with-default-model","title":"Create vLLM Service with Default Model","text":"<p>Create a single-node vLLM service with default configuration (Qwen/Qwen2.5-0.5B-Instruct):</p> <pre><code>curl -X POST http://localhost:8001/api/v1/services \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"recipe_name\": \"inference/vllm-single-node\"\n  }'\n</code></pre> <p>By default, this will use one slurm node with its 4 GPUs for tensor-parallelism. Currently, multi-node-multi-gpu is not supported, only single-node-multi-gpu. </p>"},{"location":"api/server/#create-multi-replica-vllm-service-group","title":"Create Multi-Replica vLLM Service Group","text":"<p>Create multiple vLLM replicas on a single node for high-throughput workloads. Each replica uses a subset of GPUs and listens on a different port:</p> <pre><code>curl -X POST http://localhost:8001/api/v1/services \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"recipe_name\": \"inference/vllm-replicas\"\n  }'\n</code></pre> <p>How it works: - Default: 4 GPUs with <code>gpu_per_replica: 1</code> -&gt; 4 replicas (ports 8001-8004) - Each replica runs independently on the same node - Requests are load-balanced using round-robin with automatic failover - If a replica fails, traffic routes to healthy replicas</p> <p>Customizing replicas: <pre><code>curl -X POST http://localhost:8001/api/v1/services \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"recipe_name\": \"inference/vllm-replicas\",\n    \"config\": {\n      \"resources\": {\n        \"gpu\": \"4\"\n      },\n      \"gpu_per_replica\": 2\n    }\n  }'\n</code></pre> This creates 4 replicas (4 GPUs \u00f7 2 GPUs per replica) on a single node.</p>"},{"location":"api/server/#create-vllm-service-with-custom-model","title":"Create vLLM Service with Custom Model","text":"<p>Specify a different model from HuggingFace:</p> <pre><code>curl -X POST http://localhost:8001/api/v1/services \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"recipe_name\": \"inference/vllm-single-node\",\n    \"config\": {\n      \"environment\": {\n        \"VLLM_MODEL\": \"gpt2\"\n      }\n    }\n  }'\n</code></pre>"},{"location":"api/server/#query-available-models","title":"Query Available Models","text":"<p>Before creating a service, you can search HuggingFace Hub for compatible models:</p> <pre><code># Get supported architectures and examples\ncurl http://localhost:8001/api/v1/vllm/available-models\n\n# Search for models (e.g., Qwen models)\ncurl \"http://localhost:8001/api/v1/vllm/search-models?query=qwen&amp;limit=10\"\n\n# Get detailed info about a specific model\ncurl http://localhost:8001/api/v1/vllm/model-info/Qwen/Qwen2.5-7B-Instruct\n</code></pre> <p>These endpoints return model compatibility information, download statistics, and architecture details to help you choose the right model for your use case.</p>"},{"location":"api/server/#create-vllm-service-with-custom-model-and-resources","title":"Create vLLM Service with Custom Model and Resources","text":"<p>Override both model and resource allocation (only single node supported for now):</p> <pre><code>curl -X POST http://localhost:8001/api/v1/services \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"recipe_name\": \"inference/vllm-single-node\",\n    \"config\": {\n      \"environment\": {\n        \"VLLM_MODEL\": \"gpt2\"\n      },\n      \"resources\": {\n        \"cpu\": \"8\",\n        \"memory\": \"64G\",\n        \"time_limit\": 120,\n        \"gpu\": \"4\"\n      }\n    }\n  }'\n</code></pre>"},{"location":"api/server/#create-vllm-service-with-all-options","title":"Create vLLM Service with All Options","text":"<p>Full configuration with all available environment variables and resource settings:</p> <pre><code>curl -X POST http://localhost:8001/api/v1/services \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"recipe_name\": \"inference/vllm-single-node\",\n    \"config\": {\n      \"environment\": {\n        \"VLLM_MODEL\": \"gpt2\",\n        \"VLLM_HOST\": \"0.0.0.0\",\n        \"VLLM_PORT\": \"8001\",\n        \"VLLM_MAX_MODEL_LEN\": \"2048\",\n        \"VLLM_TENSOR_PARALLEL_SIZE\": \"4\",\n        \"VLLM_GPU_MEMORY_UTILIZATION\": \"0.9\",\n        \"CUDA_VISIBLE_DEVICES\": \"0\"\n      },\n      \"resources\": {\n        \"cpu\": \"2\",\n        \"memory\": \"32G\",\n        \"time_limit\": 15,\n        \"gpu\": \"4\"\n      }\n    }\n  }'\n</code></pre>"},{"location":"api/server/#create-cpu-only-vllm-service","title":"Create CPU-Only vLLM Service","text":"<p>For testing or when GPU is not available:</p> <pre><code>curl -X POST http://localhost:8001/api/v1/services \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"recipe_name\": \"inference/vllm-single-node\",\n    \"config\": {\n      \"environment\": {\n        \"VLLM_MODEL\": \"gpt2\"\n      },\n      \"resources\": {\n        \"nodes\": 1,\n        \"cpu\": \"4\",\n        \"memory\": \"16G\",\n        \"gpu\": null\n      }\n    }\n  }'\n</code></pre>"},{"location":"api/server/#list-service-groups","title":"List Service Groups","text":"<p>Get all replica groups with their status:</p> <pre><code>curl http://localhost:8001/api/v1/service-groups\n</code></pre>"},{"location":"api/server/#get-service-group-details","title":"Get Service Group Details","text":"<p>Get information about a specific replica group:</p> <pre><code>curl http://localhost:8001/api/v1/service-groups/3652098\n</code></pre> <p>Response includes replica count, job IDs, ports, and health status.</p>"},{"location":"api/server/#get-service-group-status","title":"Get Service Group Status","text":"<p>Check aggregated status across all replicas:</p> <pre><code>curl http://localhost:8001/api/v1/service-groups/3652098/status\n</code></pre> <p>Returns: - <code>overall_status</code>: Aggregate state (all running, partially running, etc.) - <code>healthy_replicas</code>: Count of ready replicas - <code>total_replicas</code>: Total replica count - <code>replica_statuses</code>: Per-replica status and endpoints</p>"},{"location":"api/server/#stop-service-group","title":"Stop Service Group","text":"<p>Recommended method (preserves metadata for analysis):</p> <pre><code>curl -X POST http://localhost:8001/api/v1/service-groups/3652098/status \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"status\": \"cancelled\"}'\n</code></pre> <p>Response: <pre><code>{\n  \"success\": true,\n  \"message\": \"Service group 3652098 status updated to cancelled\",\n  \"group_id\": \"3652098\",\n  \"replicas_updated\": 4\n}\n</code></pre></p> <p>Legacy method (deprecated):</p> <pre><code>curl -X DELETE http://localhost:8001/api/v1/service-groups/3652098\n</code></pre> <p>Both methods cancel the underlying SLURM job(s), stopping all replicas. The POST method is preferred as it preserves service records for logging and Grafana integration.</p>"},{"location":"api/server/#list-all-services","title":"List All Services","text":"<pre><code>curl http://localhost:8001/api/v1/services\n</code></pre>"},{"location":"api/server/#get-service-details","title":"Get Service Details","text":"<pre><code>curl http://localhost:8001/api/v1/services/3652098\n</code></pre>"},{"location":"api/server/#check-service-status","title":"Check Service Status","text":"<pre><code>curl http://localhost:8001/api/v1/services/3652098/status\n</code></pre>"},{"location":"api/server/#get-service-logs","title":"Get Service Logs","text":"<pre><code>curl http://localhost:8001/api/v1/services/3652098/logs\n</code></pre>"},{"location":"api/server/#stop-a-service","title":"Stop a Service","text":"<p>Recommended method (preserves metadata for analysis):</p> <pre><code>curl -X POST http://localhost:8001/api/v1/services/3652098/status \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"status\": \"cancelled\"}'\n</code></pre> <p>Legacy method (deprecated):</p> <pre><code>curl -X DELETE http://localhost:8001/api/v1/services/3652098\n</code></pre> <p>The POST method is preferred as it preserves service records for logging and Grafana integration.</p>"},{"location":"api/server/#list-vllm-services","title":"List vLLM Services","text":"<p>Get all running vLLM services with their endpoints:</p> <pre><code>curl http://localhost:8001/api/v1/vllm/services\n</code></pre>"},{"location":"api/server/#get-available-models","title":"Get Available Models","text":"<p>Check which models are loaded in a vLLM service:</p> <pre><code>curl http://localhost:8001/api/v1/vllm/3652098/models\n</code></pre>"},{"location":"api/server/#send-prompt-to-vllm-service","title":"Send Prompt to vLLM Service","text":"<p>Send a text prompt to a single service or service group (automatically routes to a healthy replica):</p> <pre><code>curl -X POST http://localhost:8001/api/v1/vllm/3652098/prompt \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"Tell me a joke about programming\",\n    \"max_tokens\": 100\n  }'\n</code></pre> <p>Service group response includes routing information: <pre><code>{\n  \"success\": true,\n  \"text\": \"Why do programmers prefer dark mode?...\",\n  \"routed_to\": \"3652098:8002\",\n  \"group_id\": \"3652098\",\n  \"total_replicas\": 4,\n  \"usage\": {...}\n}\n</code></pre></p> <p>The <code>routed_to</code> field shows which replica handled the request (job_id:port).</p>"},{"location":"api/server/#send-prompt-with-advanced-options","title":"Send Prompt with Advanced Options","text":"<pre><code>curl -X POST http://localhost:8001/api/v1/vllm/3652098/prompt \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"Write a short haiku about AI\",\n    \"model\": \"gpt2\",\n    \"max_tokens\": 64,\n    \"temperature\": 0.7\n  }'\n</code></pre>"},{"location":"api/server/#list-available-recipes","title":"List Available Recipes","text":"<pre><code>curl http://localhost:8001/api/v1/recipes\n</code></pre>"},{"location":"api/server/#get-recipe-details","title":"Get Recipe Details","text":"<pre><code>curl http://localhost:8001/api/v1/recipes/vllm-single-node\n</code></pre>"},{"location":"api/server/#vector-db-qdrant-examples","title":"Vector DB (Qdrant) Examples","text":""},{"location":"api/server/#list-collections","title":"List Collections","text":"<p>Get the list of collections from a running Qdrant service (replace SERVICE_ID):</p> <pre><code>curl http://localhost:8001/api/v1/vector-db/3642875/collections\n</code></pre>"},{"location":"api/server/#create-collection","title":"Create Collection","text":"<p>Create a collection named <code>my_documents</code> with 384-dimensional vectors using Cosine distance:</p> <pre><code>curl -X PUT \"http://localhost:8001/api/v1/vector-db/3642875/collections/my_documents\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"vector_size\": 384, \"distance\": \"Cosine\"}'\n</code></pre>"},{"location":"api/server/#upsert-points","title":"Upsert Points","text":"<p>Insert one or more points into a collection. Each point must contain <code>id</code> and <code>vector</code> and may include a <code>payload</code>:</p> <pre><code>curl -X PUT \"http://localhost:8001/api/v1/vector-db/3642875/collections/my_documents/points\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"points\": [\n      {\"id\": 1, \"vector\": [0.1, 0.2, 0.3, 0.4], \"payload\": {\"text\": \"First document\"}},\n      {\"id\": 2, \"vector\": [0.4, 0.5, 0.6, 0.7], \"payload\": {\"text\": \"Second document\"}}\n    ]\n  }'\n</code></pre>"},{"location":"api/server/#search-points","title":"Search Points","text":"<p>Perform a similarity search against a collection (returns most similar points):</p> <pre><code>curl -X POST \"http://localhost:8001/api/v1/vector-db/3642875/collections/my_documents/points/search\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"query_vector\": [0.1, 0.2, 0.3, 0.4], \"limit\": 5}'\n</code></pre>"},{"location":"api/server/#metrics-examples","title":"Metrics Examples","text":""},{"location":"api/server/#qdrant-metrics-prometheus","title":"Qdrant Metrics (Prometheus)","text":"<p>Qdrant exposes Prometheus-compatible metrics on <code>/metrics</code>. Fetch raw metrics (text format):</p> <pre><code>curl http://localhost:8001/api/v1/vector-db/3642875/metrics\n</code></pre>"},{"location":"api/server/#vllm-metrics-prometheus","title":"vLLM Metrics (Prometheus)","text":"<p>vLLM exposes Prometheus-compatible metrics on <code>/metrics</code>. Fetch raw metrics (text format):</p> <pre><code>curl http://localhost:8001/api/v1/vllm/3642874/metrics\n</code></pre>"},{"location":"api/server/#notes","title":"Notes","text":""},{"location":"api/server/#python-examples","title":"Python examples","text":"<p>If you prefer runnable Python demos that exercise the Server API (vector DB and vLLM flows), see the <code>examples/</code> folder at the repository root. Notable scripts:</p> <ul> <li><code>examples/qdrant_simple_example.py</code></li> <li><code>examples/qdrant_simple_example_with_metrics.py</code></li> <li><code>examples/vllm_simple_example.py</code></li> <li><code>examples/vllm_simple_example_with_metrics.py</code></li> </ul> <p>Run an example locally (adjust arguments or SERVICE_ID inside the script where required):</p> <pre><code>python3 examples/vllm_simple_example.py\n</code></pre>"},{"location":"api/server/#service-behavior","title":"Service Behavior","text":"<ul> <li>Default Model: If not specified, services use <code>Qwen/Qwen2.5-0.5B-Instruct</code></li> <li>Model Sources: Models must be available on HuggingFace or cached locally</li> <li>Resource Defaults: Each recipe has default resource allocations that can be overridden</li> <li>Status Values: Services progress through states: <code>pending</code> \u2192 <code>configuring</code> \u2192 <code>running</code></li> <li>Service ID: The SLURM job ID is used as the service identifier</li> <li>Load Balancing: Replica groups use round-robin with health checks and automatic failover</li> <li>Replica Ports: Each replica gets a unique port (base_port + index), e.g., 8001, 8002, 8003, 8004</li> </ul>"},{"location":"architecture/overview/","title":"System Architecture","text":""},{"location":"architecture/overview/#overview","title":"Overview","text":"<p>The AI Factory Benchmarking Framework uses a microservices architecture to provide flexible, scalable benchmarking of AI infrastructure components on HPC systems. Importantly, the system is designed to handle an arbitrary number of benchmarks running in parallel, enabling you to orchestrate multiple concurrent experiments rather than being restricted to a single active task.</p>"},{"location":"architecture/overview/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>graph TB\n    subgraph Local[\"Local Machine (Docker Compose)\"]\n        Server[\"Server API&lt;br/&gt;:8001\"]\n        Client[\"Client API&lt;br/&gt;:8002\"]\n        Monitoring[\"Monitoring API&lt;br/&gt;:8005\"]\n        Logs[\"Logs API&lt;br/&gt;:8004\"]\n\n        Prometheus[\"Prometheus&lt;br/&gt;:9090\"]\n        Pushgateway[\"Pushgateway&lt;br/&gt;:9091\"]\n        Grafana[\"Grafana&lt;br/&gt;:3000\"]\n        Loki[\"Loki&lt;br/&gt;:3100\"]\n        Alloy[\"Alloy\"]\n    end\n\n    subgraph MeluXina[\"MeluXina HPC\"]\n        Orchestrator[\"ServiceOrchestrator&lt;br/&gt;:8000\"]\n        SLURM[\"SLURM\"]\n        ComputeNodes[\"Compute Nodes&lt;br/&gt;(vLLM, GPU Exporters)\"]\n    end\n\n    Server --&gt;|\"SSH Tunnel\"| Orchestrator\n    Client --&gt;|\"SSH + SLURM\"| SLURM\n    Orchestrator --&gt; SLURM\n    SLURM --&gt; ComputeNodes\n    ComputeNodes --&gt;|\"Push Metrics\"| Pushgateway\n    Prometheus --&gt; Pushgateway\n    Grafana --&gt; Prometheus\n    Grafana --&gt; Loki\n    Alloy --&gt; Loki\n    Logs --&gt;|\"rsync\"| MeluXina\n\n    style Server fill:#B3E5FC\n    style Grafana fill:#FFE0B2\n    style Prometheus fill:#C8E6C9</code></pre>"},{"location":"architecture/overview/#core-components","title":"Core Components","text":""},{"location":"architecture/overview/#1-server-service","title":"1. Server Service","text":"<p>Purpose: Central orchestration hub for service deployment and management.</p> <p>Overview: - Acts as a gateway proxying requests to the ServiceOrchestrator on MeluXina via SSH tunnels - Manages vLLM and vector database deployments through SLURM - Provides service discovery endpoints for Prometheus</p> <p>Learn more \u2192</p>"},{"location":"architecture/overview/#2-client-service","title":"2. Client Service","text":"<p>Purpose: Execute benchmark workloads against deployed services.</p> <p>Overview: - Submits SLURM jobs that spawn concurrent load-generating clients - Coordinates distributed load testing across compute nodes - Exports request latency and throughput metrics to Prometheus</p> <p>Learn more \u2192</p>"},{"location":"architecture/overview/#3-monitoring-service","title":"3. Monitoring Service","text":"<p>Purpose: Collect and export performance metrics.</p> <p>Overview: - Configures Prometheus scrape targets dynamically - Manages monitoring sessions with time-windowed collection - Exports metrics artifacts for offline analysis</p> <p>Learn more \u2192</p>"},{"location":"architecture/overview/#4-logging-service","title":"4. Logging Service","text":"<p>Purpose: Aggregate and forward logs for analysis.</p> <p>Overview: - Syncs SLURM job logs from MeluXina via rsync - Categorizes logs by service type (vllm, client, server, etc.) - Provides REST API for log querying and retrieval</p> <p>Learn more \u2192</p>"},{"location":"architecture/overview/#5-web-dashboard-grafana","title":"5. Web Dashboard (Grafana)","text":"<p>Purpose: Visualization and control interface.</p> <p>Overview: - Pre-configured dashboards for GPU metrics and service status - Administration panels for starting/stopping benchmarks - Real-time visualization of load test results</p>"},{"location":"architecture/overview/#data-flow","title":"Data Flow","text":""},{"location":"architecture/overview/#metrics-collection-flow","title":"Metrics Collection Flow","text":"<ol> <li>GPU Exporter runs on compute nodes, collects nvidia-smi metrics</li> <li>Metrics are pushed to Pushgateway (handles network isolation)</li> <li>Prometheus scrapes Pushgateway and service endpoints</li> <li>Grafana queries Prometheus for visualization</li> </ol>"},{"location":"architecture/overview/#log-collection-flow","title":"Log Collection Flow","text":"<ol> <li>SLURM jobs write stdout/stderr to MeluXina filesystem</li> <li>Logs Service periodically syncs via rsync/SSH</li> <li>Logs are categorized by service type</li> <li>Alloy forwards to Loki for centralized storage</li> </ol>"},{"location":"architecture/overview/#deployment-model","title":"Deployment Model","text":""},{"location":"architecture/overview/#container-orchestration","title":"Container Orchestration","text":"<p>The local stack runs via Docker Compose with the following services: - <code>server</code> (port 8001) - <code>client</code> (port 8002) - <code>logs</code> (port 8004) - <code>monitoring</code> (port 8005) - <code>prometheus</code> (port 9090) - <code>pushgateway</code> (port 9091) - <code>grafana</code> (port 3000) - <code>loki</code> (port 3100) - <code>alloy</code> (log forwarding)</p>"},{"location":"architecture/overview/#slurm-integration","title":"SLURM Integration","text":"<p>AI workloads (vLLM, load generators) run as SLURM jobs on MeluXina compute nodes with GPU allocation.</p> <p>Next: Server Service | Client Service</p>"},{"location":"development/guidelines/","title":"Development Guidelines","text":""},{"location":"development/guidelines/#ai-benchmarking-application-development-reference","title":"AI Benchmarking Application - Development Reference","text":"<p>This guide provides development standards and best practices for contributing to the AI Factory Benchmarking Framework.</p>"},{"location":"development/guidelines/#development-principles","title":"Development Principles","text":"<p>TODO...</p>"},{"location":"development/guidelines/#ci-testing","title":"CI &amp; Testing","text":"<p>This project uses a simplified CI pipeline suitable for local development and pre-merge checks:</p> <ul> <li>Each microservice contains a <code>tests/</code> folder that holds unit and integration tests specific to that microservice.</li> <li>The <code>docker-compose.test.yml</code> file orchestrates running those tests for every microservice. It is intended to be executed before merging changes into the <code>dev</code> or <code>main</code> branches.</li> </ul> <p>To run the tests locally (or in CI), use:</p> <pre><code>docker compose -f docker-compose.test.yml up --build --abort-on-container-exit\n</code></pre> <p>The test compose will bring up each test container, run the test-suite, and exit. CI should treat any non-zero exit code from the test containers as a failure and block merges until tests pass.</p>"},{"location":"development/guidelines/#local-integration-production-like-runs","title":"Local integration / production-like runs","text":"<p>For running the full application locally (a production-like scenario useful for manual testing and local benchmarking), use <code>docker-compose.yml</code>:</p> <pre><code>docker compose up --build\n</code></pre> <p>The <code>docker-compose.yml</code> setup is intended for local simulation only. For production or realistic large-scale benchmarking, services should be deployed to a proper Kubernetes (K8s) cluster that provides scheduling, scaling, and resilience.</p>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>This page provides detailed installation instructions for the AI Factory Benchmarking Framework.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Access to MeluXina supercomputer</li> <li>SLURM allocation (account: <code>p200981</code> or your project account)</li> <li>Docker and Docker Compose installed locally</li> <li>SSH access configured to MeluXina</li> <li>Git</li> </ul>"},{"location":"getting-started/installation/#installation-steps","title":"Installation Steps","text":""},{"location":"getting-started/installation/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>cd $HOME\ngit clone https://github.com/GiacomoPauletti/Benchmarking-AI-Factories.git\ncd Benchmarking-AI-Factories\n</code></pre>"},{"location":"getting-started/installation/#2-configure-environment","title":"2. Configure Environment","text":"<p>Create a <code>.env</code> file in the project root with your MeluXina credentials:</p> <pre><code>cp .env.example .env\n# Edit .env with your settings\n</code></pre>"},{"location":"getting-started/installation/#required-environment-variables","title":"Required environment variables:","text":""},{"location":"getting-started/installation/#ssh-configuration","title":"SSH Configuration:","text":"<ul> <li><code>SSH_HOST</code> - MeluXina hostname (e.g., <code>login.lxp.lu</code>)</li> <li><code>SSH_PORT</code> - SSH port (<code>8822</code> for MeluXina)</li> <li><code>SSH_USER</code> - Your MeluXina username (e.g., <code>u103056</code>)</li> <li><code>SSH_KEY_PATH</code> - Path to your SSH private key (e.g., <code>~/.ssh/id_ed25519</code>)</li> </ul>"},{"location":"getting-started/installation/#remote-paths","title":"Remote Paths:","text":"<ul> <li><code>REMOTE_BASE_PATH</code> - Working directory on MeluXina (e.g., <code>~/ai-factory-benchmarks</code>)</li> <li><code>REMOTE_HF_CACHE_PATH</code> - HuggingFace model cache directory (e.g., <code>/project/scratch/p200981/u103056/huggingface_cache</code>)</li> <li><code>REMOTE_SIF_DIR</code> - Singularity image storage directory (e.g., <code>/project/scratch/p200981/u103056</code>)</li> </ul>"},{"location":"getting-started/installation/#huggingface","title":"HuggingFace:","text":"<ul> <li><code>HF_TOKEN</code> - HuggingFace API token for accessing gated models and higher rate limits</li> </ul>"},{"location":"getting-started/installation/#slurm-configuration","title":"SLURM Configuration:","text":"<ul> <li><code>ORCHESTRATOR_ACCOUNT</code> - SLURM account (e.g., <code>p200981</code>)</li> <li><code>ORCHESTRATOR_PARTITION</code> - Default partition (e.g., <code>cpu</code>)</li> <li><code>ORCHESTRATOR_QOS</code> - Quality of Service (e.g., <code>default</code>)</li> </ul>"},{"location":"getting-started/installation/#example-configuration","title":"Example configuration:","text":"<pre><code># MeluXina SSH Configuration\nSSH_HOST=login.lxp.lu\nSSH_PORT=8822\nSSH_USER=u103056\nSSH_KEY_PATH=~/.ssh/id_ed25519\n\n# HuggingFace Authentication\nHF_TOKEN=hf_your_token_here\n\n# Remote Paths\nREMOTE_BASE_PATH=~/ai-factory-benchmarks\nREMOTE_HF_CACHE_PATH=/project/scratch/p200981/u103056/huggingface_cache\nREMOTE_SIF_DIR=/project/scratch/p200981/u103056\n\n# SLURM Configuration\nORCHESTRATOR_ACCOUNT=p200981\nORCHESTRATOR_PARTITION=cpu\n</code></pre>"},{"location":"getting-started/installation/#3-configure-ssh-agent","title":"3. Configure SSH Agent","text":"<p>The framework uses SSH agent forwarding for secure authentication without exposing private keys to containers.</p> <p>SSH Agent Security</p> <p>SSH agent forwarding is more secure than mounting raw SSH keys because:</p> <ul> <li>Private keys never enter the container filesystem</li> <li>Authentication happens via the agent on your host machine</li> <li>Supports multiple keys and respects your <code>~/.ssh/config</code></li> <li>Follows the principle of least privilege</li> </ul> <p>Ensure your SSH agent is running:</p> <pre><code># Check if SSH agent is running\necho $SSH_AUTH_SOCK\n\n# If empty, start the agent (usually automatic on desktop environments)\neval \"$(ssh-agent -s)\"\n\n# Add your MeluXina SSH key to the agent\nssh-add ~/.ssh/id_ed25519  # Or your key file\n\n# Verify key is loaded\nssh-add -l\n</code></pre> <p>Desktop Environments</p> <p>Most Linux desktop environments (GNOME, KDE, etc.) automatically start an SSH agent. You typically only need to run <code>ssh-add</code> once after login.</p> <p>The Docker containers will use the <code>SSH_AUTH_SOCK</code> environment variable to communicate with your host's SSH agent.</p>"},{"location":"getting-started/installation/#4-start-the-application","title":"4. Start the Application","text":"<pre><code>docker compose up -d \n</code></pre> <p>Once all services are running, you can access the Grafana dashboard at:</p> <p>http://localhost:3000</p> <p>Available Services</p> <p>After starting the application, the following services will be available:</p> <ul> <li>Grafana Dashboard: http://localhost:3000</li> <li>Server API: http://localhost:8001/docs</li> <li>Client API: http://localhost:8002/docs</li> <li>Logs API: http://localhost:8004/docs</li> <li>Monitoring API: http://localhost:8005/docs</li> <li>Prometheus: http://localhost:9090</li> </ul>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Overview - Learn about the framework</li> <li>Architecture - Understand the system architecture</li> <li>Server API Documentation - Explore the Server API endpoints</li> <li>Client API Documentation - Explore the Client API endpoints</li> <li>Logs API Documentation - Explore the Logs API endpoints</li> <li>Monitoring API Documentation - Explore the Monitoring API endpoints</li> </ul> <p>Note: the repository uses <code>docker compose</code> for local development and testing. For realistic benchmarking and production deployments these services should run on a Kubernetes (K8s) cluster instead. The docker-compose setup is intended for local testing only.</p>"},{"location":"getting-started/overview/","title":"Getting Started","text":"<p>This guide will help you set up and run the AI Factory Benchmarking Framework on the MeluXina supercomputer.</p>"},{"location":"getting-started/overview/#overview","title":"Overview","text":"<p>The AI Factory Benchmarking Framework is a distributed system for deploying, benchmarking, and monitoring AI inference services on HPC clusters. Unlike simple tools that run sequentially, our framework lets you launch an arbitrary number of benchmarks in parallel, allowing you to scale your experiments without being limited to one benchmark at a time.</p> <p>Features:</p> <ul> <li>Deploy dozens of vLLM replicas via SLURM with GPU allocation, automatically load balanced</li> <li>Run distributed load tests with hundreds of clients performing load on the vLLM services</li> <li>Collect GPU metrics (utilization, power, temperature) in real-time</li> <li>Visualize performance through Grafana dashboards</li> <li>Aggregate and query SLURM job logs</li> </ul> <p>All this through a simple, intuitive Grafana UI.</p>"},{"location":"getting-started/overview/#installation","title":"Installation","text":"<p>To install and set up the framework, see the Installation Guide.</p>"},{"location":"getting-started/overview/#getting-help","title":"Getting Help","text":"<ul> <li>Check API documentations</li> <li>GitHub Issues: Report a bug</li> </ul> <p>Continue to Installation Guide for setup instructions!</p>"},{"location":"services/client/","title":"Client Service","text":""},{"location":"services/client/#overview","title":"Overview","text":"<p>The Client Service manages distributed load testing and benchmarking of AI inference services on HPC clusters. It orchestrates client groups that generate realistic workloads against vLLM and other AI services deployed via the Server.</p> <p>Live API Explorer</p> <p>The best way to explore the API is here:</p> <p>Open Interactive API Docs</p>"},{"location":"services/client/#what-it-does","title":"What It Does","text":"<p>The Client Service has the following functionalities:</p> <ul> <li>Submit SLURM jobs that spawn multiple concurrent clients</li> <li>Create groups of clients that generate coordinated load</li> <li>Export Prometheus metrics for real-time monitoring</li> <li>Collect and save detailed performance results</li> <li>Discover and monitor running client groups</li> </ul>"},{"location":"services/client/#architecture","title":"Architecture","text":"<p>The Client Service works in conjunction with the Server to create end-to-end benchmarking pipelines:</p> <pre><code>graph TB\n    subgraph Local[\"Local Machine\"]\n        User[\"User/Script\"]\n        Server[\"Server API&lt;br/&gt;localhost:8001\"]\n        ClientSvc[\"Client API&lt;br/&gt;localhost:8002\"]\n    end\n\n    subgraph MeluXina[\"MeluXina Supercomputer\"]\n        Orch[\"ServiceOrchestrator&lt;br/&gt;Port 8000\"]\n        SLURM[\"SLURM Scheduler\"]\n\n        subgraph ComputeNode1[\"Compute Node A\"]\n            vLLM[\"vLLM Service&lt;br/&gt;Port 8001\"]\n        end\n\n        subgraph ComputeNode2[\"Compute Node B\"]\n            LoadGen[\"Load Generator&lt;br/&gt;(Client Group)\"]\n        end\n    end\n\n    User --&gt;|\"1. Create vLLM\"| Server\n    Server --&gt;|SSH| Orch\n    Orch --&gt;|sbatch| SLURM\n    SLURM --&gt;|schedule| vLLM\n\n    User --&gt;|\"2. Create Client Group\"| ClientSvc\n    ClientSvc --&gt;|SSH + sbatch| SLURM\n    SLURM --&gt;|schedule| LoadGen\n\n    LoadGen --&gt;|\"3. Generate Load\"| vLLM\n    User --&gt;|\"4. Query Metrics\"| ClientSvc\n    ClientSvc --&gt;|proxy| LoadGen\n\n    style Server fill:#B3E5FC,stroke:#0288D1,stroke-width:2px\n    style ClientSvc fill:#C8E6C9,stroke:#388E3C,stroke-width:2px\n    style vLLM fill:#FFE0B2,stroke:#F57C00,stroke-width:2px\n    style LoadGen fill:#F8BBD0,stroke:#C2185B,stroke-width:2px</code></pre>"},{"location":"services/client/#concepts","title":"Concepts","text":""},{"location":"services/client/#client-groups","title":"Client Groups","text":"<p>A client group is a collection of concurrent clients running on HPC compute nodes. Each group:</p> <ul> <li>Runs as a SLURM job on MeluXina</li> <li>Contains N concurrent client threads/processes</li> <li>Generates load at a target rate (requests per second)</li> <li>Reports metrics via Prometheus</li> <li>Saves detailed results to log files</li> </ul>"},{"location":"services/client/#workflow","title":"Workflow","text":"<ol> <li>Create Service: User creates a vLLM service via Server API</li> <li>Wait for Ready: Service starts and models are loaded</li> <li>Create Client Group: User creates a client group via Client API</li> <li>Generate Load: Clients send concurrent requests to the vLLM service</li> </ol>"},{"location":"services/client/#load-test-configuration","title":"Load Test Configuration","text":"<p>Each client group is configured with:</p> <ul> <li><code>service_id</code>: ID of the target vLLM service (or group of services)</li> <li><code>num_clients</code>: Number of concurrent client threads (1-1000)</li> <li><code>requests_per_second</code>: Target RPS across all clients</li> <li><code>duration_seconds</code>: How long to run the test</li> <li><code>prompts</code>: List of prompts to randomly sample from</li> <li><code>max_tokens</code>: Maximum tokens to generate per request</li> <li><code>temperature</code>: Sampling temperature (0.0-2.0)</li> <li><code>time_limit</code>: SLURM job time limit in minutes</li> </ul>"},{"location":"services/client/#components","title":"Components","text":"<pre><code>classDiagram\n    class FastAPIApp:::api {\n        +health()\n        +create_client_group()\n        +list_client_groups()\n        +get_client_group_info()\n    }\n\n    class ClientManager:::manager {\n        +add_client_group(id, config)\n        +remove_client_group(id)\n        +list_groups()\n        +get_group_info(id)\n        +run_client_group(id)\n    }\n\n    class ClientGroup:::group {\n        -group_id: int\n        -load_config: dict\n        -dispatcher: SlurmClientDispatcher\n        +get_status()\n        +get_client_address()\n        +register_observer(url)\n    }\n\n    class SlurmClientDispatcher:::slurm {\n        +dispatch(group_id, time_limit)\n        +_submit_slurm_job_via_ssh()\n        +_generate_sbatch_script()\n    }\n\n    class SSHManager:::ssh {\n        +http_request_via_ssh()\n        +execute_remote_command()\n        +sync_logs_from_remote()\n    }\n\n    FastAPIApp --&gt; ClientManager\n    ClientManager --&gt; ClientGroup\n    ClientGroup --&gt; SlurmClientDispatcher\n    SlurmClientDispatcher --&gt; SSHManager\n\n    classDef api fill:#C8E6C9,stroke:#388E3C,stroke-width:2px\n    classDef manager fill:#FFE0B2,stroke:#F57C00,stroke-width:2px\n    classDef group fill:#F8BBD0,stroke:#C2185B,stroke-width:2px\n    classDef slurm fill:#E1BEE7,stroke:#8E24AA,stroke-width:2px\n    classDef ssh fill:#E0E0E0,stroke:#9E9E9E,stroke-width:1px</code></pre>"},{"location":"services/client/#component-responsibilities","title":"Component Responsibilities","text":"<p>FastAPI App: REST API endpoints for external clients</p> <p>ClientManager: Singleton that manages all client groups   - Tracks active groups by ID   - Coordinates group creation/deletion   - Resolves orchestrator endpoint from Server</p> <p>ClientGroup: Represents a single SLURM job with N clients   - Stores load test configuration   - Tracks registration status   - Provides access to client process metrics</p> <p>SlurmClientDispatcher: Submits SLURM jobs via SSH   - Generates sbatch scripts   - Submits jobs to SLURM scheduler   - Handles job configuration (time limits, resources)</p> <p>SSHManager: SSH communication with MeluXina   - Executes remote commands   - Syncs log files   - Manages SSH tunnels</p>"},{"location":"services/client/#api-reference","title":"API Reference","text":"<p>See Client API Documentation for detailed endpoint documentation and interactive Swagger UI.</p>"},{"location":"services/client/#testing","title":"Testing","text":"<p>The Client Service includes tests:</p> <pre><code># Run unit tests\n./services/client/run_tests.sh unit\n\n# Run integration tests (requires live services)\n./services/client/run_tests.sh integration\n\n# Run all tests\n./services/client/run_tests.sh all\n</code></pre> <p>See the Testing Documentation for more details.</p>"},{"location":"services/logs/","title":"Logs Service","text":""},{"location":"services/logs/#overview","title":"Overview","text":"<p>The Logs Service provides centralized log collection and management for all SLURM jobs running on MeluXina. It automatically syncs logs from the HPC cluster, categorizes them by service type, and exposes them via REST API.</p> <p>Live API Explorer</p> <p>The best way to explore the API is here:</p> <p>Open Interactive API Docs</p>"},{"location":"services/logs/#what-it-does","title":"What It Does","text":"<ul> <li>Automatic Sync: Periodically syncs logs from MeluXina to local storage</li> <li>Categorization: Organizes logs by service type (server, client, vllm, vector-db, etc.)</li> <li>REST API: Query and retrieve logs via HTTP endpoints</li> <li>Statistics: Provides file counts and size metrics per service</li> <li>Manual Triggers: Force immediate sync via API</li> </ul>"},{"location":"services/logs/#architecture","title":"Architecture","text":"<pre><code>graph LR\n    subgraph MeluXina\n        Jobs[\"SLURM Jobs&lt;br/&gt;(*.out, *.err)\"]\n    end\n\n    subgraph \"Logs Service\"\n        Sync[\"Background Sync&lt;br/&gt;(rsync via SSH)\"]\n        Cat[\"Log Categorizer\"]\n        API[\"REST API&lt;br/&gt;Port 8004\"]\n        Storage[\"Local Storage&lt;br/&gt;/app/data\"]\n    end\n\n    User[\"User/Client\"]\n\n    Jobs --&gt;|\"rsync\"| Sync\n    Sync --&gt; Storage\n    Storage --&gt; Cat\n    Cat --&gt; Storage\n    User --&gt;|\"Query logs\"| API\n    API --&gt; Storage\n\n    style API fill:#C8E6C9,stroke:#388E3C,stroke-width:2px\n    style Storage fill:#FFF9C4,stroke:#FBC02D,stroke-width:2px</code></pre> <p>Logs are automatically categorized into:</p> <ul> <li>server: Service orchestrator logs</li> <li>client: Load generator logs (loadgen-*.out/err)</li> <li>vllm: vLLM inference service logs</li> <li>vector-db: Vector database logs (Qdrant, etc.)</li> <li>monitoring: Prometheus exporter logs</li> <li>uncategorized: Unrecognized log files</li> </ul>"},{"location":"services/logs/#periodic-sync","title":"Periodic Sync","text":"<p>The service runs a background task that:</p> <ol> <li>Syncs logs from <code>~/ai-factory-benchmarks/logs/</code> on MeluXina</li> <li>Categorizes them by pattern matching</li> <li>Updates statistics</li> <li>Repeats every N seconds (configurable via <code>SYNC_INTERVAL</code>)</li> </ol>"},{"location":"services/logs/#usage-examples","title":"Usage Examples","text":""},{"location":"services/logs/#list-all-services","title":"List All Services","text":"<pre><code>curl http://localhost:8004/services\n</code></pre> <p>Response: <pre><code>[\"server\", \"client\", \"vllm\", \"vector-db\", \"monitoring\", \"uncategorized\"]\n</code></pre></p>"},{"location":"services/logs/#get-service-statistics","title":"Get Service Statistics","text":"<pre><code>curl http://localhost:8004/services/stats\n</code></pre> <p>Response: <pre><code>{\n  \"client\": {\n    \"service\": \"client\",\n    \"file_count\": 24,\n    \"total_size_bytes\": 1048576,\n    \"latest_modified\": \"2024-11-25T15:30:00\"\n  },\n  \"vllm\": {\n    \"service\": \"vllm\",\n    \"file_count\": 8,\n    \"total_size_bytes\": 524288,\n    \"latest_modified\": \"2024-11-25T15:25:00\"\n  }\n}\n</code></pre></p>"},{"location":"services/logs/#list-logs-for-a-service","title":"List Logs for a Service","text":"<pre><code># Get all client logs\ncurl \"http://localhost:8004/logs?service=client&amp;limit=10\"\n</code></pre> <p>Response: <pre><code>[\n  {\n    \"filename\": \"loadgen-12345.out\",\n    \"path\": \"categorized/client/loadgen-12345.out\",\n    \"size_bytes\": 45632,\n    \"modified_time\": \"2024-11-25T15:30:00\",\n    \"service\": \"client\"\n  }\n]\n</code></pre></p>"},{"location":"services/logs/#get-log-content","title":"Get Log Content","text":"<pre><code># Get last 100 lines of a log file\ncurl \"http://localhost:8004/logs/content?path=categorized/client/loadgen-12345.out&amp;tail=100\"\n</code></pre>"},{"location":"services/logs/#trigger-manual-sync","title":"Trigger Manual Sync","text":"<pre><code>curl -X POST http://localhost:8004/sync/trigger\n</code></pre> <p>Response: <pre><code>{\n  \"status\": \"success\",\n  \"message\": \"Log sync completed\",\n  \"categories\": {\n    \"client\": 5,\n    \"vllm\": 2,\n    \"uncategorized\": 1\n  }\n}\n</code></pre></p>"},{"location":"services/logs/#delete-logs","title":"Delete Logs","text":"<pre><code># Delete specific log file\ncurl -X POST \"http://localhost:8004/logs/delete?path=categorized/client/loadgen-12345.out\"\n\n# Delete all logs (cleanup)\ncurl -X POST http://localhost:8004/logs/cleanup\n</code></pre>"},{"location":"services/logs/#configuration","title":"Configuration","text":"<p>Environment variables:</p> <ul> <li><code>SYNC_INTERVAL</code>: Seconds between syncs (default: 60)</li> <li><code>REMOTE_BASE_PATH</code>: Base path on MeluXina (default: ~/ai-factory-benchmarks)</li> <li><code>SSH_HOST</code>, <code>SSH_USER</code>, <code>SSH_PORT</code>: SSH connection details</li> <li><code>LOG_LEVEL</code>: Logging verbosity (default: INFO)</li> </ul>"},{"location":"services/logs/#api-reference","title":"API Reference","text":"<p>See Logs API Documentation for complete endpoint documentation.</p>"},{"location":"services/monitoring/","title":"Monitoring Service","text":""},{"location":"services/monitoring/#overview","title":"Overview","text":"<p>The Monitoring Service collects and aggregates performance metrics from deployed AI services and the HPC infrastructure. It integrates with Prometheus, Pushgateway, and Grafana to provide real-time visibility into GPU utilization, power consumption, and service health.</p> <p>Live API Explorer</p> <p>The best way to explore the API is here:</p> <p>Open Interactive API Docs</p>"},{"location":"services/monitoring/#what-it-does","title":"What It Does","text":"<ul> <li>Metrics Collection: Gathers GPU metrics (utilization, memory, power, temperature) from compute nodes</li> <li>Pushgateway Integration: Buffers metrics when direct scraping isn't possible (network isolation)</li> <li>Target Registration: Dynamically registers services and exporters for Prometheus scraping</li> <li>Session Management: Create monitoring sessions with configurable time windows</li> <li>Artifact Generation: Export collected metrics as CSV/JSON for analysis</li> </ul>"},{"location":"services/monitoring/#architecture","title":"Architecture","text":"<pre><code>graph TB\n    subgraph Local[\"Local Machine\"]\n        Monitoring[\"Monitoring API&lt;br/&gt;localhost:8005\"]\n        Prometheus[\"Prometheus&lt;br/&gt;localhost:9090\"]\n        Pushgateway[\"Pushgateway&lt;br/&gt;localhost:9091\"]\n        Grafana[\"Grafana&lt;br/&gt;localhost:3000\"]\n    end\n\n    subgraph MeluXina[\"MeluXina Compute Nodes\"]\n        GPUExporter[\"GPU Exporter&lt;br/&gt;(nvidia-smi metrics)\"]\n        vLLM[\"vLLM Service\"]\n    end\n\n    GPUExporter --&gt;|\"Push metrics\"| Pushgateway\n    Prometheus --&gt;|\"Scrape\"| Pushgateway\n    Prometheus --&gt;|\"Proxy via Server\"| vLLM\n    Grafana --&gt;|\"Query\"| Prometheus\n    Monitoring --&gt;|\"Configure targets\"| Prometheus\n\n    style Monitoring fill:#C8E6C9,stroke:#388E3C,stroke-width:2px\n    style Prometheus fill:#FFE0B2,stroke:#F57C00,stroke-width:2px\n    style Grafana fill:#B3E5FC,stroke:#0288D1,stroke-width:2px</code></pre>"},{"location":"services/monitoring/#api-reference","title":"API Reference","text":"<p>See Monitoring API Documentation for the interactive API reference.</p>"},{"location":"services/server/","title":"Server API Reference","text":"<p>The Server Service provides REST API for managing AI services on the MeluXina supercomputer.</p>"},{"location":"services/server/#interactive-api-documentation","title":"Interactive API Documentation","text":"<p>Live API Explorer</p> <p>The best way to explore the API is here:</p> <p>Open Interactive API Docs</p>"},{"location":"services/server/#overview","title":"Overview","text":"<p>A FastAPI gateway server that proxies AI workload orchestration requests to a ServiceOrchestrator running on MeluXina.</p>"},{"location":"services/server/#architecture","title":"Architecture","text":"<p>The system consists of two main components:</p> <ol> <li>Server (Gateway): Runs locally (e.g., laptop/workstation), exposes REST API, forwards requests via SSH</li> <li>ServiceOrchestrator: Runs on MeluXina, manages SLURM jobs, handles container deployment</li> </ol>"},{"location":"services/server/#what-it-does","title":"What it does","text":"<ul> <li>API Gateway: Provides a stable REST API endpoint for external clients</li> <li>Request Proxying: Forwards all orchestration requests to the ServiceOrchestrator via SSH tunnels</li> <li>Dual Access Pattern: </li> <li>External clients \u2192 Server (localhost:8001) \u2192 SSH tunnel \u2192 Orchestrator (MeluXina)</li> <li>Internal clients (on MeluXina) \u2192 Orchestrator API directly (orchestrator:8000)</li> </ul>"},{"location":"services/server/#deployment-architecture","title":"Deployment Architecture","text":"<p>The system is split across two locations with clear separation of concerns:</p> <pre><code>graph TB\n    subgraph Local[\"Local Machine (Laptop/Workstation)\"]\n        Client[\"Client Application\"]\n        Server[\"Server (Gateway)&lt;br/&gt;FastAPI&lt;br/&gt;Port 8001\"]\n    end\n\n    subgraph MeluXina[\"MeluXina Supercomputer\"]\n        Orchestrator[\"ServiceOrchestrator&lt;br/&gt;FastAPI&lt;br/&gt;Port 8000\"]\n        SLURM[\"SLURM Cluster\"]\n        Compute[\"Compute Nodes&lt;br/&gt;(vLLM, Qdrant, etc.)\"]\n    end\n\n    Client --&gt;|\"HTTP: localhost:8001\"| Server\n    Server --&gt;|\"SSH Tunnel&lt;br/&gt;(http_request_via_ssh)\"| Orchestrator\n    Orchestrator --&gt;|\"sbatch/scancel\"| SLURM\n    SLURM --&gt;|\"Job Control\"| Compute\n    Orchestrator --&gt;|\"HTTP&lt;br/&gt;(status, models, prompts)\"| Compute\n\n    InternalClient[\"Internal Client&lt;br/&gt;(on MeluXina)\"] -.-&gt;|\"HTTP: orchestrator:8000&lt;br/&gt;(Direct Access)\"| Orchestrator\n\n    style Server fill:#B3E5FC,stroke:#0288D1,stroke-width:2px\n    style Orchestrator fill:#C8E6C9,stroke:#388E3C,stroke-width:2px\n    style Client fill:#FFF9C4,stroke:#FBC02D,stroke-width:1px\n    style InternalClient fill:#FFF9C4,stroke:#FBC02D,stroke-width:1px,stroke-dasharray: 5 5</code></pre> <p>Key Points:</p> <ul> <li>Server (Gateway): Lightweight proxy running locally, provides stable API endpoint</li> <li>ServiceOrchestrator: Heavy lifting component on MeluXina, manages SLURM and containers</li> <li>SSH Tunnel: Secure communication channel between server and orchestrator</li> <li>Dual Access: External clients use the gateway; internal MeluXina clients can bypass it</li> </ul>"},{"location":"services/server/#server-gateway-components","title":"Server (Gateway) Components","text":"<p>The server component focuses on request proxying and client-facing API:</p> <pre><code>classDiagram\n    class FastAPIApp:::api {\n        +root()\n        +health()\n    }\n\n    class APIRouter:::api {\n        +create_service()\n        +list_services()\n        +stop_service()\n        +get_service_status()\n        +update_service_status()\n        +vllm_endpoints()\n        +vector_db_endpoints()\n    }\n\n    class OrchestratorProxy:::proxy {\n        +start_service(recipe, config)\n        +stop_service(service_id)\n        +list_services()\n        +get_service_status(service_id)\n        +prompt_vllm_service(...)\n        +get_vllm_models(...)\n        +_make_request(method, endpoint)\n    }\n\n    class SSHManager:::ssh {\n        +http_request_via_ssh(host, port, method, path)\n        +execute_remote_command(cmd)\n        +setup_tunnel(local_port, remote_port)\n    }\n\n    FastAPIApp --&gt; APIRouter\n    APIRouter --&gt; OrchestratorProxy\n    OrchestratorProxy --&gt; SSHManager\n\n    classDef api fill:#B3E5FC,stroke:#0288D1,stroke-width:2px\n    classDef proxy fill:#FFE0B2,stroke:#F57C00,stroke-width:2px\n    classDef ssh fill:#E0E0E0,stroke:#9E9E9E,stroke-width:1px</code></pre> <p>Server Layer Responsibilities:</p> <ul> <li>Expose REST API on <code>localhost:8001</code> for external clients</li> <li>Parse and validate client requests</li> <li>Forward requests to orchestrator via SSH tunnels</li> <li>Handle SSH connection failures gracefully</li> <li>Return normalized responses to clients</li> </ul>"},{"location":"services/server/#serviceorchestrator-components-on-meluxina","title":"ServiceOrchestrator Components (on MeluXina)","text":"<p>The orchestrator handles all the heavy lifting:</p> <pre><code>classDiagram\n    class OrchestratorAPI:::api {\n        +start_service()\n        +stop_service()\n        +list_services()\n        +get_service_group()\n    }\n\n    class ServiceOrchestrator:::core {\n        +start_service(recipe_name, config)\n        +stop_service(service_id)\n        +update_service_status(service_id, status)\n        +stop_service_group(group_id)\n        +update_service_group_status(group_id, status)\n    }\n\n    class SlurmDeployer:::infra {\n        +submit_job()\n        +cancel_job()\n        +get_job_status()\n    }\n\n    class ServiceManager:::infra {\n        +register_service()\n        +get_service()\n        +update_service_status()\n    }\n\n    class RecipeLoader:::infra {\n        +load(recipe_name)\n        +list_all()\n    }\n\n    class BuilderRegistry:::infra {\n        +create_builder(category)\n    }\n\n    OrchestratorAPI --&gt; ServiceOrchestrator\n    ServiceOrchestrator *-- SlurmDeployer\n    ServiceOrchestrator *-- ServiceManager\n    ServiceOrchestrator *-- RecipeLoader\n    SlurmDeployer --&gt; BuilderRegistry\n\n    classDef api fill:#C8E6C9,stroke:#388E3C,stroke-width:2px\n    classDef core fill:#DCEDC8,stroke:#8BC34A,stroke-width:2px\n    classDef infra fill:#F5F5F5,stroke:#9E9E9E,stroke-width:1px</code></pre> <p>Orchestrator Layer Responsibilities:</p> <ul> <li>Expose internal API on <code>orchestrator:8000</code> (accessible from MeluXina network)</li> <li>Load and parse recipe YAML files</li> <li>Generate SLURM job scripts with appropriate resource allocations</li> <li>Submit jobs to SLURM via <code>sbatch</code></li> <li>Track service lifecycle (pending \u2192 configuring \u2192 running \u2192 cancelled)</li> <li>Resolve compute node endpoints for running services</li> <li>Query service health and models from compute nodes</li> <li>Handle service groups (replica sets) and load balancing</li> </ul>"},{"location":"services/server/#service-handlers-types","title":"Service Handlers &amp; Types","text":"<pre><code>graph TD\n    %% =========================================\n    %% Base / Abstract layer\n    %% =========================================\n    BaseService[\"BaseService\"]\n\n    %% =========================================\n    %% Subgraphs / Layers\n    %% =========================================\n    subgraph InferenceLayer[\"Inference Layer\"]\n        InferenceService[\"InferenceService\"]\n        VllmService[\"VllmService\"]\n    end\n\n    subgraph VectorDBLayer[\"VectorDB Layer\"]\n        VectorDbService[\"VectorDbService\"]\n        QdrantService[\"QdrantService\"]\n    end\n\n    subgraph InfraLayer[\"Infrastructure Layer\"]\n        SlurmDeployer[\"SlurmDeployer\"]\n        EndpointResolver[\"EndpointResolver\"]\n    end\n\n    %% =========================================\n    %% Inheritance / Composition\n    %% =========================================\n    BaseService --&gt; InferenceService\n    InferenceService --&gt; VllmService\n    BaseService --&gt; VectorDbService\n    VectorDbService --&gt; QdrantService\n\n    %% =========================================\n    %% Dependencies\n    %% =========================================\n    VllmService --&gt; EndpointResolver\n    QdrantService --&gt; EndpointResolver\n    VllmService --&gt; SlurmDeployer\n    QdrantService --&gt; SlurmDeployer\n\n    %% =========================================\n    %% Styling\n    %% =========================================\n    class BaseService base\n    class InferenceService inference\n    class VllmService inference\n    class VectorDbService vectordb\n    class QdrantService vectordb\n    class SlurmDeployer infra\n    class EndpointResolver infra\n\n    classDef base fill:#FFF9C4,stroke:#FBC02D,stroke-width:1px,color:#795548\n    classDef inference fill:#DCEDC8,stroke:#8BC34A,stroke-width:1px,color:#33691E\n    classDef vectordb fill:#E1BEE7,stroke:#8E24AA,stroke-width:1px,color:#4A148C\n    classDef infra fill:#E0E0E0,stroke:#9E9E9E,stroke-width:1px,color:#424242\n</code></pre> <p>The service-handlers diagram explains how domain-specific functionality is organized:</p> <ul> <li><code>BaseService</code> provides the shared plumbing (deployer access, service registry, endpoint resolution) used by concrete handlers.</li> <li><code>InferenceService</code> and <code>VectorDbService</code> define the operations expected by their domains; <code>VllmService</code> and <code>QdrantService</code> implement those operations against running jobs.</li> <li>These handlers consult <code>SlurmDeployer</code> for live job state and <code>EndpointResolver</code> to discover the compute-node HTTP endpoints used to reach the actual running services.</li> </ul> <p>Refer to this diagram when extending the system with a new service type (create a subclass of <code>BaseService</code> and implement the domain-specific API surface).</p>"},{"location":"services/server/#how-recipe-yaml-files-work","title":"How Recipe YAML Files Work","text":"<p>Recipe YAML files define service configurations that are loaded and used to generate SLURM job scripts for deploying services on the cluster.</p>"},{"location":"services/server/#recipe-loading-flow","title":"Recipe Loading Flow","text":"<pre><code>User Request \u2192 RecipeLoader.load(recipe_name) \u2192 YAML File Read \u2192 Recipe Object \u2192 SlurmDeployer\n                                                                                         \u2193\nBuilderRegistry.create_builder() \u2192 RecipeScriptBuilder \u2192 Generated SLURM Script \u2192 sbatch\n</code></pre>"},{"location":"services/server/#recipe-yaml-structure","title":"Recipe YAML Structure","text":"<p>Each recipe YAML file contains:</p> Section Purpose <code>name</code> Recipe identifier (e.g., <code>vllm</code>) <code>category</code> Service type: <code>inference</code>, <code>vector-db</code>, or <code>storage</code> <code>description</code> Human-readable service description <code>image</code> Singularity/Apptainer image filename (built from <code>.def</code>) <code>container_def</code> Singularity definition file for building the image <code>ports</code> Default ports the service exposes <code>environment</code> Environment variables passed to the container <code>resources</code> Default SLURM resource requests (can be overridden per job) <code>distributed</code> Configuration for multi-node/multi-GPU execution <code>replicas</code> Number of independent service instances for data parallelism (optional)"},{"location":"services/server/#example-vllm-recipe","title":"Example: vLLM Recipe","text":"<pre><code>name: vllm-single-node\ncategory: inference\ndescription: \"vLLM high-performance inference server for large language models\"\nversion: \"0.2.0\"\nimage: \"vllm.sif\"\ncontainer_def: \"vllm.def\"\n\nports:\n  - 8001\n\nenvironment:\n  VLLM_HOST: \"0.0.0.0\"\n  VLLM_PORT: \"8001\"\n  VLLM_MODEL: \"Qwen/Qwen2.5-0.5B-Instruct\"\n  VLLM_WORKDIR: \"/workspace\"\n  VLLM_LOGGING_LEVEL: \"INFO\"\n  VLLM_TENSOR_PARALLEL_SIZE: \"4\" \n\nresources:\n  nodes: \"1\"\n  cpu: \"2\"\n  memory: \"32G\"\n  time_limit: 15\n  gpu: \"4\"\n</code></pre>"},{"location":"services/server/#multi-replica-configuration","title":"Multi-Replica Configuration","text":"<p>The <code>vllm-replicas</code> recipe creates multiple replicas on a single node for high-throughput inference:</p> <pre><code>name: vllm-replicas\ncategory: inference\ndescription: \"vLLM with multiple replicas - flexible GPU allocation per replica\"\n\nenvironment:\n  VLLM_HOST: \"0.0.0.0\"\n  VLLM_MODEL: \"Qwen/Qwen2.5-0.5B-Instruct\"\n  # ... other settings ...\n\nresources:  # Per node (not per replica)\n  nodes: \"1\"  # Number of nodes to allocate\n  cpu: \"8\"    # CPUs per node\n  memory: \"64G\"  # Memory per node\n  time_limit: 15\n  gpu: \"4\"    # Total GPUs per node\n\n# Replica group configuration\n# System calculates: replicas_per_node = gpu / gpu_per_replica\ngpu_per_replica: 1  # Each replica uses 1 GPU (data parallel)\nbase_port: 8001     # First replica uses 8001, second uses 8002, etc.\n</code></pre> <p>How it works: - With <code>gpu: 4</code> and <code>gpu_per_replica: 1</code>, you get 4 replicas - Each replica runs independently on separate GPUs (0, 1, 2, 3) - Replicas listen on consecutive ports (8001, 8002, 8003, 8004) - All replicas run in a single SLURM job - Load balancing distributes requests using round-robin with automatic failover</p>"},{"location":"services/server/#how-recipes-are-used","title":"How Recipes Are Used","text":"<ol> <li>Service Creation: User calls <code>/api/v1/services</code> with <code>recipe_name: \"inference/vllm-single-node\"</code></li> <li>Recipe Loading: <code>RecipeLoader</code> reads <code>recipes/inference/vllm-single-node.yaml</code></li> <li>Configuration Merge: User-provided config (e.g., <code>gpu: 8</code>) overrides recipe defaults</li> <li>Replica Detection: If <code>gpu_per_replica</code> field is present, calculate replicas per node</li> <li>Builder Selection: <code>BuilderRegistry</code> selects recipe-specific builder or category default</li> <li>Script Generation: Builder generates SLURM script with replica configuration</li> <li>Job Submission: Submit single SLURM job that launches all replicas on assigned GPUs</li> </ol>"},{"location":"services/server/#recipe-specific-builders","title":"Recipe-Specific Builders","text":"<p>Some recipes have custom builders that override script generation behavior. For example:</p> <ul> <li><code>VllmInferenceBuilder</code>: Overrides <code>build_distributed_run_block()</code> to add tensor parallelism with <code>torchrun</code></li> <li><code>QdrantVectorDbBuilder</code>: Overrides <code>build_run_block()</code> to mount job-specific storage paths</li> </ul> <p>This allows recipes to customize script generation without modifying the core <code>SlurmDeployer</code>.</p>"},{"location":"services/server/#recipe-script-builders-orchestrator-component","title":"Recipe Script Builders (Orchestrator Component)","text":"<p>The following diagram shows the Recipe Builder architecture running on the ServiceOrchestrator (MeluXina side). It uses the Strategy pattern to generate SLURM job scripts for different recipe types. This modular design allows adding new services without modifying the core SLURM deployer.</p> <pre><code>classDiagram\n    %% =========================================\n    %% Abstract Base\n    %% =========================================\n    class RecipeScriptBuilder:::base {\n        &lt;&lt;abstract&gt;&gt;\n        +build_environment_section()*\n        +build_container_build_block()*\n        +build_run_block()*\n        +supports_distributed()\n        +build_distributed_run_block()*\n    }\n\n    class BuilderRegistry:::registry {\n        +register(category, builder_class)\n        +register_recipe(recipe_name, builder_class)\n        +create_builder(category, recipe_name)\n        +list_categories()\n        +list_recipes()\n    }\n\n    %% =========================================\n    %% Category Builders (Generic)\n    %% =========================================\n    class InferenceRecipeBuilder:::inference {\n        +build_environment_section()\n        +build_container_build_block()\n        +build_run_block()\n        +supports_distributed() bool\n        +build_distributed_run_block()\n    }\n\n    class VectorDbRecipeBuilder:::vectordb {\n        +build_environment_section()\n        +build_container_build_block()\n        +build_run_block()\n        +supports_distributed() bool\n    }\n\n    %% =========================================\n    %% Recipe-Specific Builders\n    %% =========================================\n    class VllmInferenceBuilder:::inference {\n        +build_distributed_run_block()\n    }\n\n    class QdrantVectorDbBuilder:::vectordb {\n        +build_run_block()\n    }\n\n    %% =========================================\n    %% Client\n    %% =========================================\n    class SlurmDeployer:::client {\n        +_create_script(recipe, config)\n    }\n\n    %% =========================================\n    %% Relationships\n    %% =========================================\n    RecipeScriptBuilder &lt;|-- InferenceRecipeBuilder\n    RecipeScriptBuilder &lt;|-- VectorDbRecipeBuilder\n    InferenceRecipeBuilder &lt;|-- VllmInferenceBuilder\n    VectorDbRecipeBuilder &lt;|-- QdrantVectorDbBuilder\n\n    BuilderRegistry ..&gt; RecipeScriptBuilder : creates\n    SlurmDeployer --&gt; BuilderRegistry : uses\n\n    %% =========================================\n    %% Styling\n    %% =========================================\n    classDef base fill:#FFF9C4,stroke:#FBC02D,stroke-width:2px,color:#795548\n    classDef registry fill:#FFE0B2,stroke:#F57C00,stroke-width:2px,color:#E65100\n    classDef inference fill:#DCEDC8,stroke:#8BC34A,stroke-width:1px,color:#33691E\n    classDef vectordb fill:#E1BEE7,stroke:#8E24AA,stroke-width:1px,color:#4A148C\n    classDef client fill:#E0E0E0,stroke:#9E9E9E,stroke-width:1px,color:#424242</code></pre>"},{"location":"services/server/#further-reading","title":"Further Reading","text":"<ul> <li>Service Recipes - Available service templates</li> <li>Architecture - System design</li> <li>Development Guide - API development</li> </ul>"}]}