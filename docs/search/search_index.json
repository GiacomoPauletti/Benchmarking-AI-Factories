{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AI Factory Benchmarking Framework","text":"<p>Welcome to the AI Factory Benchmarking Framework documentation.</p>"},{"location":"#overview","title":"Overview","text":"<p>This framework enables benchmarking of AI Factory components on HPC systems, specifically designed for the MeluXina supercomputer. The system orchestrates AI workloads via SLURM and provides comprehensive monitoring and testing capabilities.</p>"},{"location":"#architecture","title":"Architecture","text":"<p>The framework consists of four microservices:</p> <pre><code>graph TB\n    Client[Client Service&lt;br/&gt;Benchmark Executor] --&gt; Server[Server Service&lt;br/&gt;SLURM Orchestration]\n    Monitor[Monitoring Service&lt;br/&gt;Metrics Collection] --&gt; Server\n    Logs[Logs Service&lt;br/&gt;Log Aggregation] --&gt; Server\n\n    Server --&gt; SLURM[SLURM REST API]\n    SLURM --&gt; HPC[MeluXina HPC]\n\n    style Server fill:#4051b5\n    style Client fill:#e8eaf6\n    style Monitor fill:#e8eaf6\n    style Logs fill:#e8eaf6</code></pre>"},{"location":"#services","title":"Services","text":"<ul> <li>Server - Core orchestration service that manages SLURM job submissions and AI workload deployment</li> <li>Client - Executes benchmark tests and workload simulations (TODO: Documentation in progress)</li> <li>Monitoring - Collects and aggregates performance metrics (TODO: Documentation in progress)</li> <li>Logs - Centralized log collection and analysis (TODO: Documentation in progress)</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>See Getting Started for detailed instructions.</p>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Getting Started - Setup and installation</li> <li>Architecture - System design and components</li> <li>Server Service - Detailed server documentation</li> <li>API Reference - Interactive API documentation</li> <li>Development - Development guidelines</li> </ul>"},{"location":"#status","title":"Status","text":"<p>Current Version: 1.0.0 Last Updated: October 2025 Project: EUMaster4HPC Challenge 2025-2026</p>"},{"location":"api/client/","title":"Client API Documentation","text":""},{"location":"api/client/#status-todo","title":"Status: TODO","text":"<p>This page will contain the interactive API reference for the Client Service.</p> <p></p>"},{"location":"api/client/#overview","title":"Overview","text":"<p>The Client Service will provide endpoints for interacting with the Benchmarking AI Factories platform from external applications.</p> <p>Note: This service is currently under development. Check back soon for complete documentation.</p>"},{"location":"api/logs/","title":"Logs API Documentation","text":""},{"location":"api/logs/#status-todo","title":"Status: TODO","text":"<p>This page will contain the interactive API reference for the Logs Service.</p> <p></p>"},{"location":"api/logs/#overview","title":"Overview","text":"<p>The Logs Service will provide endpoints for centralized log aggregation, search, and analysis.</p> <p>Note: This service is currently under development. Check back soon for complete documentation.</p>"},{"location":"api/monitoring/","title":"Monitoring API Documentation","text":""},{"location":"api/monitoring/#status-todo","title":"Status: TODO","text":"<p>This page will contain the interactive API reference for the Monitoring Service.</p> <p></p>"},{"location":"api/monitoring/#overview","title":"Overview","text":"<p>The Monitoring Service will provide endpoints for tracking system metrics, job performance, and resource utilization.</p> <p>Note: This service is currently under development. Check back soon for complete documentation.</p>"},{"location":"api/server/","title":"Server API Documentation","text":""},{"location":"api/server/#interactive-api-reference","title":"Interactive API Reference","text":"<p>The Server Service provides a REST API for managing SLURM jobs and AI workload orchestration.</p> <p></p>"},{"location":"api/server/#api-examples","title":"API Examples","text":""},{"location":"api/server/#create-vllm-service-with-default-model","title":"Create vLLM Service with Default Model","text":"<p>Create a vLLM service with default configuration (Qwen/Qwen2.5-0.5B-Instruct):</p> <pre><code>curl -X POST http://localhost:8001/api/v1/services \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"recipe_name\": \"inference/vllm\"\n  }'\n</code></pre>"},{"location":"api/server/#create-vllm-service-with-custom-model","title":"Create vLLM Service with Custom Model","text":"<p>Specify a different model from HuggingFace:</p> <pre><code>curl -X POST http://localhost:8001/api/v1/services \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"recipe_name\": \"inference/vllm\",\n    \"config\": {\n      \"environment\": {\n        \"VLLM_MODEL\": \"gpt2\"\n      }\n    }\n  }'\n</code></pre>"},{"location":"api/server/#create-vllm-service-with-custom-model-and-resources","title":"Create vLLM Service with Custom Model and Resources","text":"<p>Override both model and resource allocation:</p> <pre><code>curl -X POST http://localhost:8001/api/v1/services \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"recipe_name\": \"inference/vllm\",\n    \"config\": {\n      \"environment\": {\n        \"VLLM_MODEL\": \"gpt2\"\n      },\n      \"resources\": {\n        \"nodes\": 1,\n        \"cpu\": \"8\",\n        \"memory\": \"64G\",\n        \"time_limit\": 120,\n        \"gpu\": \"1\"\n      }\n    }\n  }'\n</code></pre>"},{"location":"api/server/#create-vllm-service-with-all-options","title":"Create vLLM Service with All Options","text":"<p>Full configuration with all available environment variables and resource settings:</p> <pre><code>curl -X POST http://localhost:8001/api/v1/services \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"recipe_name\": \"inference/vllm\",\n    \"config\": {\n      \"environment\": {\n        \"VLLM_MODEL\": \"gpt2\",\n        \"VLLM_HOST\": \"0.0.0.0\",\n        \"VLLM_PORT\": \"8001\",\n        \"VLLM_MAX_MODEL_LEN\": \"2048\",\n        \"VLLM_TENSOR_PARALLEL_SIZE\": \"1\",\n        \"VLLM_GPU_MEMORY_UTILIZATION\": \"0.9\",\n        \"CUDA_VISIBLE_DEVICES\": \"0\"\n      },\n      \"resources\": {\n        \"nodes\": 1,\n        \"cpu\": \"2\",\n        \"memory\": \"32G\",\n        \"time_limit\": 15,\n        \"gpu\": \"1\"\n      }\n    }\n  }'\n</code></pre>"},{"location":"api/server/#create-cpu-only-vllm-service","title":"Create CPU-Only vLLM Service","text":"<p>For testing or when GPU is not available:</p> <pre><code>curl -X POST http://localhost:8001/api/v1/services \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"recipe_name\": \"inference/vllm\",\n    \"config\": {\n      \"environment\": {\n        \"VLLM_MODEL\": \"gpt2\"\n      },\n      \"resources\": {\n        \"nodes\": 1,\n        \"cpu\": \"4\",\n        \"memory\": \"16G\",\n        \"gpu\": null\n      }\n    }\n  }'\n</code></pre>"},{"location":"api/server/#list-all-services","title":"List All Services","text":"<pre><code>curl http://localhost:8001/api/v1/services\n</code></pre>"},{"location":"api/server/#get-service-details","title":"Get Service Details","text":"<pre><code>curl http://localhost:8001/api/v1/services/3652098\n</code></pre>"},{"location":"api/server/#check-service-status","title":"Check Service Status","text":"<pre><code>curl http://localhost:8001/api/v1/services/3652098/status\n</code></pre>"},{"location":"api/server/#get-service-logs","title":"Get Service Logs","text":"<pre><code>curl http://localhost:8001/api/v1/services/3652098/logs\n</code></pre>"},{"location":"api/server/#stop-a-service","title":"Stop a Service","text":"<pre><code>curl -X DELETE http://localhost:8001/api/v1/services/3652098\n</code></pre>"},{"location":"api/server/#list-vllm-services","title":"List vLLM Services","text":"<p>Get all running vLLM services with their endpoints:</p> <pre><code>curl http://localhost:8001/api/v1/vllm/services\n</code></pre>"},{"location":"api/server/#get-available-models","title":"Get Available Models","text":"<p>Check which models are loaded in a vLLM service:</p> <pre><code>curl http://localhost:8001/api/v1/vllm/3652098/models\n</code></pre>"},{"location":"api/server/#send-prompt-to-vllm-service","title":"Send Prompt to vLLM Service","text":"<p>Send a text prompt and get a response:</p> <pre><code>curl -X POST http://localhost:8001/api/v1/vllm/3652098/prompt \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"Tell me a joke about programming\",\n    \"max_tokens\": 100\n  }'\n</code></pre>"},{"location":"api/server/#send-prompt-with-advanced-options","title":"Send Prompt with Advanced Options","text":"<pre><code>curl -X POST http://localhost:8001/api/v1/vllm/3652098/prompt \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"Write a short haiku about AI\",\n    \"model\": \"gpt2\",\n    \"max_tokens\": 64,\n    \"temperature\": 0.7\n  }'\n</code></pre>"},{"location":"api/server/#list-available-recipes","title":"List Available Recipes","text":"<pre><code>curl http://localhost:8001/api/v1/recipes\n</code></pre>"},{"location":"api/server/#get-recipe-details","title":"Get Recipe Details","text":"<pre><code>curl http://localhost:8001/api/v1/recipes/inference/vllm\n</code></pre>"},{"location":"api/server/#notes","title":"Notes","text":"<ul> <li>Default Model: If not specified, services use <code>Qwen/Qwen2.5-0.5B-Instruct</code></li> <li>Model Sources: Models must be available on HuggingFace or cached locally</li> <li>Resource Defaults: Each recipe has default resource allocations that can be overridden</li> <li>Status Values: Services progress through states: <code>pending</code> \u2192 <code>building</code> \u2192 <code>starting</code> \u2192 <code>running</code></li> <li>Service ID: The SLURM job ID is used as the service identifier</li> </ul>"},{"location":"architecture/overview/","title":"System Architecture","text":""},{"location":"architecture/overview/#overview","title":"Overview","text":"<p>The AI Factory Benchmarking Framework uses a microservices architecture to provide flexible, scalable benchmarking of AI infrastructure components on HPC systems.</p>"},{"location":"architecture/overview/#high-level-architecture","title":"High-Level Architecture","text":"<p>TODO...</p>"},{"location":"architecture/overview/#core-components","title":"Core Components","text":""},{"location":"architecture/overview/#1-server-service","title":"1. Server Service","text":"<p>Purpose: Central orchestration hub for service deployment and management.</p> <p>Overview:</p> <p>Learn more \u2192</p>"},{"location":"architecture/overview/#2-client-service","title":"2. Client Service","text":"<p>Purpose: Execute benchmark workloads against deployed services.</p> <p>TODO...</p>"},{"location":"architecture/overview/#3-monitoring-service","title":"3. Monitoring Service","text":"<p>Purpose: Collect and export performance metrics.</p> <p>TODO...</p>"},{"location":"architecture/overview/#4-logging-service","title":"4. Logging Service","text":"<p>Purpose: Aggregate and forward logs for analysis.</p> <p>TODO...</p>"},{"location":"architecture/overview/#5-web-dashboard","title":"5. Web Dashboard","text":"<p>Purpose: Visualization and control interface.</p> <p>TODO...</p>"},{"location":"architecture/overview/#data-flow","title":"Data Flow","text":""},{"location":"architecture/overview/#service-deployment-flow","title":"Service Deployment Flow","text":"<p>TODO...</p>"},{"location":"architecture/overview/#monitoring-data-flow","title":"Monitoring Data Flow","text":"<p>TODO...</p>"},{"location":"architecture/overview/#deployment-model","title":"Deployment Model","text":""},{"location":"architecture/overview/#container-orchestration","title":"Container Orchestration","text":"<p>TODO...</p>"},{"location":"architecture/overview/#slurm-integration","title":"SLURM Integration","text":"<p>TODO...</p>"},{"location":"architecture/overview/#communication-patterns","title":"Communication Patterns","text":""},{"location":"architecture/overview/#inter-service-communication","title":"Inter-Service Communication","text":"<p>TODO...</p> <p>Next: Microservices Details | Container Orchestration</p>"},{"location":"development/guidelines/","title":"Development Guidelines","text":""},{"location":"development/guidelines/#ai-benchmarking-application-development-reference","title":"AI Benchmarking Application - Development Reference","text":"<p>This guide provides development standards and best practices for contributing to the AI Factory Benchmarking Framework.</p>"},{"location":"development/guidelines/#development-principles","title":"Development Principles","text":"<p>TODO...</p>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>This page provides detailed installation instructions for the AI Factory Benchmarking Framework.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Access to MeluXina supercomputer</li> <li>SLURM allocation (account: <code>p200981</code> or your project account)</li> <li>Docker and Docker Compose installed locally</li> <li>SSH access configured to MeluXina</li> <li>Git</li> </ul>"},{"location":"getting-started/installation/#installation-steps","title":"Installation Steps","text":""},{"location":"getting-started/installation/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>cd $HOME\ngit clone https://github.com/GiacomoPauletti/Benchmarking-AI-Factories.git\ncd Benchmarking-AI-Factories\n</code></pre>"},{"location":"getting-started/installation/#2-configure-environment","title":"2. Configure Environment","text":"<p>Create a <code>.env</code> file in the project root with your MeluXina credentials:</p> <pre><code>cp .env.example .env\n# Edit .env with your settings\n</code></pre> <p>Required environment variables: - <code>SSH_HOST</code> - MeluXina hostname - <code>SSH_PORT</code> - SSH port (typically 22) - <code>SSH_USER</code> - Your MeluXina username - <code>SSH_KEY_PATH</code> - Path to your SSH private key - <code>REMOTE_BASE_PATH</code> - Working directory on MeluXina</p>"},{"location":"getting-started/installation/#3-start-the-microservice","title":"3. Start the Microservice","text":"<p>TODO: Later, this will be replaced by the actual app using the microservices.</p> <pre><code>docker compose up -d &lt;microservice to launch&gt;\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Overview - Learn about the framework</li> <li>Architecture - Understand the system architecture</li> <li>Server API Documentation - Explore the Server API endpoints</li> <li>Client API Documentation - Explore the Client API endpoints</li> <li>Logs API Documentation - Explore the Logs API endpoints</li> <li>Monitoring API Documentation - Explore the Monitoring API endpoints</li> </ul>"},{"location":"getting-started/overview/","title":"Getting Started","text":"<p>This guide will help you set up and run the AI Factory Benchmarking Framework on the MeluXina supercomputer.</p>"},{"location":"getting-started/overview/#overview","title":"Overview","text":"<p>TODO: Project overview will be written here.</p> <p>This section will provide: - What is the AI Factory Benchmarking Framework - Key features and capabilities - Use cases and scenarios - Architecture at a glance</p>"},{"location":"getting-started/overview/#installation","title":"Installation","text":"<p>To install and set up the framework, see the Installation Guide.</p>"},{"location":"getting-started/overview/#getting-help","title":"Getting Help","text":"<ul> <li>Check API documentations</li> <li>GitHub Issues: Report a bug</li> </ul> <p>Continue to Installation Guide for setup instructions!</p>"},{"location":"services/client/","title":"Client Service","text":""},{"location":"services/client/#overview","title":"Overview","text":"<p>The Client Service executes benchmark tests and workload simulations against deployed AI services.</p>"},{"location":"services/client/#status","title":"Status","text":"<p>TODO: This service is under development. Documentation will be added by the team member responsible for the Client service.</p>"},{"location":"services/client/#planned-features","title":"Planned Features","text":"<ul> <li>Benchmark execution engine</li> <li>Workload generation</li> <li>Performance metrics collection</li> <li>Test result aggregation</li> </ul>"},{"location":"services/client/#api-reference","title":"API Reference","text":"<p>See Client API Documentation for the interactive API reference.</p>"},{"location":"services/client/#contact","title":"Contact","text":"<p>For questions about the Client service, please contact the responsible team member.</p>"},{"location":"services/logs/","title":"Logs Service","text":""},{"location":"services/logs/#overview","title":"Overview","text":"<p>The Logs Service provides centralized log collection, aggregation, and analysis for all services and SLURM jobs.</p>"},{"location":"services/logs/#status","title":"Status","text":"<p>TODO: This service is under development. Documentation will be added by the team member responsible for the Logs service.</p>"},{"location":"services/logs/#planned-features","title":"Planned Features","text":"<ul> <li>Centralized log collection</li> <li>Log aggregation and indexing</li> <li>Query interface for log analysis</li> <li>Integration with log management platforms (Loki, Elasticsearch)</li> </ul>"},{"location":"services/logs/#api-reference","title":"API Reference","text":"<p>See Logs API Documentation for the interactive API reference.</p>"},{"location":"services/logs/#contact","title":"Contact","text":"<p>For questions about the Logs service, please contact the responsible team member.</p>"},{"location":"services/monitoring/","title":"Monitoring Service","text":""},{"location":"services/monitoring/#overview","title":"Overview","text":"<p>The Monitoring Service collects and aggregates performance metrics from deployed AI services and the HPC infrastructure.</p>"},{"location":"services/monitoring/#status","title":"Status","text":"<p>TODO: This service is under development. Documentation will be added by the team member responsible for the Monitoring service.</p>"},{"location":"services/monitoring/#planned-features","title":"Planned Features","text":"<ul> <li>Metrics collection from SLURM jobs</li> <li>Performance monitoring</li> <li>Resource utilization tracking</li> <li>Integration with monitoring platforms (Prometheus, Grafana)</li> </ul>"},{"location":"services/monitoring/#api-reference","title":"API Reference","text":"<p>See Monitoring API Documentation for the interactive API reference.</p>"},{"location":"services/monitoring/#contact","title":"Contact","text":"<p>For questions about the Monitoring service, please contact the responsible team member.</p>"},{"location":"services/server/api-reference/","title":"Server API Reference","text":"<p>The Server Service provides a comprehensive REST API for managing AI services on the MeluXina supercomputer.</p>"},{"location":"services/server/api-reference/#interactive-api-documentation","title":"Interactive API Documentation","text":"<p>Live API Explorer</p> <p>The best way to explore the API is through the interactive Swagger UI:</p> <p>Open Interactive API Docs</p> <p>(Replace <code>localhost:8001</code> with your actual server endpoint)</p>"},{"location":"services/server/api-reference/#overview","title":"Overview","text":"<p>TODO: Explain how service works from a technical point of view.</p>"},{"location":"services/server/api-reference/#further-reading","title":"Further Reading","text":"<ul> <li>Service Recipes - Available service templates</li> <li>Architecture - System design</li> <li>Development Guide - API development</li> </ul>"}]}