{"openapi":"3.1.0","info":{"title":"AI Factory Server Service","description":"SLURM + Apptainer orchestration for AI workloads","version":"1.0.0"},"paths":{"/":{"get":{"summary":"Root","description":"Root endpoint with service information.","operationId":"root__get","responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}}}}},"/health":{"get":{"summary":"Health","description":"Health check endpoint.","operationId":"health_health_get","responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}}}}},"/api/v1/services":{"post":{"summary":"Create and start a new service","description":"Create and start a new service using SLURM + Apptainer.\n\nThis endpoint submits a job to the SLURM cluster using a predefined recipe template.\nThe service will be containerized using Apptainer and scheduled on compute nodes.\n\n**Request Body:**\n- `recipe_name` (required): Path to the recipe (e.g., \"inference/vllm\", \"inference/vllm_dummy\")\n- `config` (optional): Configuration object with:\n    - **SLURM resource requirements** (all optional, override recipe defaults):\n        - `nodes`: Number of compute nodes (default: 1)\n        - `resources`: Resource overrides object:\n            - `cpu`: Number of CPUs (e.g., \"2\", \"8\")\n            - `memory`: Memory per CPU (e.g., \"8G\", \"16G\", \"32G\")\n            - `time_limit`: Time limit in minutes (e.g., 15, 60, 120)\n            - `gpu`: GPU allocation (e.g., \"1\", \"2\", null for CPU-only)\n    - **Environment variables** (all optional, override recipe defaults):\n        - `environment`: Environment variable overrides:\n            - `VLLM_MODEL`: Model to load (e.g., \"gpt2\", \"Qwen/Qwen2.5-0.5B-Instruct\")\n            - `VLLM_MAX_MODEL_LEN`: Max sequence length\n            - `VLLM_GPU_MEMORY_UTILIZATION`: GPU memory fraction (0.0-1.0)\n            - Any other environment variables supported by the container\n\n**Examples:**\n\nSimple creation with defaults:\n```json\n{\n  \"recipe_name\": \"inference/vllm\"\n}\n```\n\nCustom model:\n```json\n{\n  \"recipe_name\": \"inference/vllm\",\n  \"config\": {\n    \"environment\": {\n      \"VLLM_MODEL\": \"gpt2\"\n    }\n  }\n}\n```\n\nCustom model + resources:\n```json\n{\n  \"recipe_name\": \"inference/vllm\",\n  \"config\": {\n    \"nodes\": 1,\n    \"environment\": {\n      \"VLLM_MODEL\": \"meta-llama/Llama-2-7b-chat-hf\"\n    },\n    \"resources\": {\n      \"cpu\": \"8\",\n      \"memory\": \"64G\",\n      \"time_limit\": 120,\n      \"gpu\": \"1\"\n    }\n  }\n}\n```\n\n**Returns:**\n- Service object with `id` (SLURM job ID), `name`, `status`, `recipe_name`, `config`, and `created_at`\n\n**Status Values:**\n- `pending`: Job queued in SLURM\n- `building`: Apptainer container image being built\n- `starting`: Container started, application initializing\n- `running`: Service fully operational and ready\n- `completed`: Service finished successfully\n- `failed`: Service encountered an error\n- `cancelled`: Service was stopped by user","operationId":"create_service_api_v1_services_post","requestBody":{"required":true,"content":{"application/json":{"schema":{"allOf":[{"$ref":"#/components/schemas/ServiceRequest"}],"examples":{"simple":{"summary":"Create a basic vLLM service","value":{"recipe_name":"inference/vllm","config":{"nodes":1,"cpus":2,"memory":"8G","time":"00:30:00"}}}},"title":"Request"}}}},"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{"$ref":"#/components/schemas/ServiceResponse"}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}},"get":{"summary":"List Services","description":"List all services managed by this server.\n\nReturns all services that were started through this API. Does not include\nunrelated SLURM jobs (e.g., interactive sessions, other users' jobs).\n\n**Returns:**\n- Array of service objects, each containing:\n    - `id`: SLURM job ID (used as service identifier)\n    - `name`: Service name (derived from recipe)\n    - `status`: Current status (pending/building/starting/running/completed/failed/cancelled)\n    - `recipe_name`: Recipe used to create the service\n    - `config`: Resource configuration (nodes, cpus, memory, time, etc.)\n    - `created_at`: Service creation timestamp\n\n**Example Response:**\n```json\n[\n  {\n    \"id\": \"3642874\",\n    \"name\": \"vllm-service\",\n    \"status\": \"running\",\n    \"recipe_name\": \"inference/vllm\",\n    \"config\": {\"nodes\": 1, \"cpus\": 4, \"memory\": \"16G\"},\n    \"created_at\": \"2025-10-14T10:30:00\"\n  }\n]\n```","operationId":"list_services_api_v1_services_get","responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{"type":"array","items":{"$ref":"#/components/schemas/ServiceResponse"},"title":"Response List Services Api V1 Services Get"}}}}}}},"/api/v1/services/{service_id}":{"get":{"summary":"Get Service","description":"Get detailed information about a specific service.\n\n**Path Parameters:**\n- `service_id`: The SLURM job ID of the service (obtained from create_service or list_services)\n\n**Returns:**\n- Service object with current status and configuration details\n\n**Errors:**\n- 404: Service not found (not managed by this server)\n\n**Example:**\n- GET `/api/v1/services/3642874`","operationId":"get_service_api_v1_services__service_id__get","parameters":[{"name":"service_id","in":"path","required":true,"schema":{"type":"string","title":"Service Id"}}],"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{"$ref":"#/components/schemas/ServiceResponse"}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}},"delete":{"summary":"Stop Service","description":"Stop a running service by cancelling its SLURM job.\n\nThis endpoint cancels the SLURM job associated with the service, which will:\n1. Terminate the running container\n2. Free up allocated compute resources\n3. Mark the service as cancelled in SLURM\n\n**Path Parameters:**\n- `service_id`: The SLURM job ID of the service to stop\n\n**Returns:**\n- Success message with service ID\n\n**Errors:**\n- 404: Service not found or already stopped\n\n**Example:**\n- DELETE `/api/v1/services/3642874`\n\n**Note:** This operation is immediate and cannot be undone. The service will be terminated gracefully.","operationId":"stop_service_api_v1_services__service_id__delete","parameters":[{"name":"service_id","in":"path","required":true,"schema":{"type":"string","title":"Service Id"}}],"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}}},"/api/v1/services/{service_id}/logs":{"get":{"summary":"Get Service Logs","description":"Get SLURM logs (stdout and stderr) from a service.\n\nRetrieves the job output logs written by SLURM. These logs contain:\n- Container build output (if applicable)\n- Application startup messages\n- Runtime logs and errors\n- Container termination information\n\n**Path Parameters:**\n- `service_id`: The SLURM job ID of the service\n\n**Returns:**\n- Object with `logs` field containing the combined stdout/stderr output\n\n**Example Response:**\n```json\n{\n  \"logs\": \"=== SLURM STDOUT ===\\nBuilding Apptainer image...\\nStarting container...\\nApplication startup complete.\"\n}\n```\n\n**Note:** Logs may not be available immediately after job creation. They become available once the job starts running.","operationId":"get_service_logs_api_v1_services__service_id__logs_get","parameters":[{"name":"service_id","in":"path","required":true,"schema":{"type":"string","title":"Service Id"}}],"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}}},"/api/v1/services/{service_id}/status":{"get":{"summary":"Get Service Status","description":"Get the current detailed status of a service.\n\nThis endpoint returns the real-time status by checking both SLURM state and parsing log files\nto determine the exact stage of service initialization.\n\n**Path Parameters:**\n- `service_id`: The SLURM job ID of the service\n\n**Returns:**\n- Object with `status` field containing one of:\n    - `pending`: Job waiting in SLURM queue\n    - `building`: Apptainer container image being built\n    - `starting`: Container launched, application initializing\n    - `running`: Service fully operational\n    - `completed`: Service finished successfully\n    - `failed`: Service encountered an error\n    - `cancelled`: Service was stopped\n    - `unknown`: Unable to determine status\n\n**Example Response:**\n```json\n{\n  \"status\": \"running\"\n}\n```","operationId":"get_service_status_api_v1_services__service_id__status_get","parameters":[{"name":"service_id","in":"path","required":true,"schema":{"type":"string","title":"Service Id"}}],"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}}},"/api/v1/recipes":{"get":{"summary":"List Recipes","description":"List all available service recipes.\n\nRecipes are YAML templates that define how to deploy services on SLURM.\nEach recipe specifies the container image, resource requirements, and runtime configuration.\n\n**Returns:**\n- Array of recipe objects, each containing:\n    - `name`: Recipe identifier (e.g., \"vLLM Inference Service\")\n    - `category`: Recipe category (e.g., \"inference\", \"storage\", \"vector-db\")\n    - `description`: Human-readable description\n    - `version`: Recipe version\n    - `path`: Path to access the recipe (e.g., \"inference/vllm\")\n\n**Example Response:**\n```json\n[\n  {\n    \"name\": \"vLLM Inference Service\",\n    \"category\": \"inference\",\n    \"description\": \"High-performance LLM inference server\",\n    \"version\": \"1.0.0\",\n    \"path\": \"inference/vllm\"\n  }\n]\n```\n\n**Recipe Categories:**\n- `inference`: ML model inference services (vLLM, TensorFlow Serving, etc.)\n- `storage`: Data storage solutions\n- `vector-db`: Vector database services for RAG applications\n- `simple`: Basic test/demo services","operationId":"list_recipes_api_v1_recipes_get","responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{"items":{"$ref":"#/components/schemas/RecipeResponse"},"type":"array","title":"Response List Recipes Api V1 Recipes Get"}}}}}}},"/api/v1/recipes/{recipe_name}":{"get":{"summary":"Get Recipe","description":"Get detailed information about a specific recipe.\n\n**Path Parameters:**\n- `recipe_name`: Recipe name or path (e.g., \"vLLM Inference Service\" or \"inference/vllm\")\n\n**Returns:**\n- Recipe object with name, category, description, version, and path\n\n**Errors:**\n- 404: Recipe not found\n\n**Example:**\n- GET `/api/v1/recipes/inference/vllm`","operationId":"get_recipe_api_v1_recipes__recipe_name__get","parameters":[{"name":"recipe_name","in":"path","required":true,"schema":{"type":"string","title":"Recipe Name"}}],"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{"$ref":"#/components/schemas/RecipeResponse"}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}}},"/api/v1/vllm/services":{"get":{"summary":"List Vllm Services","description":"List all running vLLM inference services with their endpoints.\n\nThis endpoint discovers vLLM services among all running services and resolves their network endpoints.\nUse this to find available vLLM instances for inference requests.\n\n**Returns:**\n- Object with `vllm_services` array, each service containing:\n    - `id`: SLURM job ID (service identifier)\n    - `name`: Service name\n    - `recipe_name`: Recipe used (typically \"inference/vllm\")\n    - `endpoint`: HTTP endpoint URL (e.g., \"http://mel2133:8001\")\n    - `status`: Current status (building/starting/running)\n\n**Example Response:**\n```json\n{\n  \"vllm_services\": [\n    {\n      \"id\": \"3642874\",\n      \"name\": \"vllm-service\",\n      \"recipe_name\": \"inference/vllm\",\n      \"endpoint\": \"http://mel2133:8001\",\n      \"status\": \"running\"\n    }\n  ]\n}\n```\n\n**Status Meanings:**\n- `building`: Container image being built\n- `starting`: vLLM server initializing, not ready for requests\n- `running`: vLLM fully loaded and ready to serve inference requests\n\n**Note:** Only services with status \"running\" are ready to accept prompt requests.","operationId":"list_vllm_services_api_v1_vllm_services_get","responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}}}}},"/api/v1/vllm/{service_id}/prompt":{"post":{"summary":"Send a prompt to a running vLLM service","description":"Send a text prompt to a running vLLM inference service and get a response.\n\nThis endpoint forwards your prompt to the vLLM service using the OpenAI-compatible API.\nThe vLLM service must be in \"running\" status (not \"building\" or \"starting\").\n\n**Path Parameters:**\n- `service_id`: SLURM job ID of the vLLM service (from list_vllm_services)\n\n**Request Body:**\n- `prompt` (required): Text prompt to send to the model\n- `model` (optional): Model identifier (auto-discovered if omitted)\n- `max_tokens` (optional): Maximum tokens to generate (default: 150)\n- `temperature` (optional): Sampling temperature 0.0-2.0 (default: 0.7)\n\n**Returns (Success):**\n```json\n{\n  \"success\": true,\n  \"response\": \"Generated text response from the model...\",\n  \"service_id\": \"3642874\",\n  \"endpoint\": \"http://mel2133:8001\",\n  \"usage\": {\"prompt_tokens\": 10, \"completion_tokens\": 25, \"total_tokens\": 35}\n}\n```\n\n**Returns (Error):**\n```json\n{\n  \"success\": false,\n  \"error\": \"Failed to connect to VLLM service: Connection refused\",\n  \"endpoint\": \"http://mel2133:8001\"\n}\n```\n\n**Errors:**\n- 400: Prompt is missing or invalid\n- 404: vLLM service not found\n- 500: Service error or connection failure\n\n**Example:**\n```bash\ncurl -X POST \"http://server:8000/api/v1/vllm/3642874/prompt\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"prompt\": \"What is AI?\", \"max_tokens\": 100}'\n```\n\n**Note:** The vLLM service must be fully initialized (status=\"running\") before it can accept prompts.","operationId":"prompt_vllm_service_api_v1_vllm__service_id__prompt_post","parameters":[{"name":"service_id","in":"path","required":true,"schema":{"type":"string","title":"Service Id"}}],"requestBody":{"required":true,"content":{"application/json":{"schema":{"type":"object","examples":{"simple":{"summary":"Basic prompt","value":{"prompt":"Write a short haiku about AI."}},"with_model":{"summary":"Prompt specifying model","value":{"prompt":"Hello","model":"gpt2","max_tokens":64}}},"title":"Request"}}}},"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}}},"/api/v1/vllm/{service_id}/models":{"get":{"summary":"Get Vllm Models","description":"Get the list of models served by a running vLLM service.\n\nQueries the vLLM service's /v1/models endpoint to discover which models are loaded and available.\nThis is useful when you don't know which model to specify in prompt requests.\n\n**Path Parameters:**\n- `service_id`: SLURM job ID of the vLLM service\n\n**Returns:**\n```json\n{\n  \"models\": [\"Qwen/Qwen3-0.6B\", \"gpt2\"]\n}\n```\n\n**Returns (No Models):**\n```json\n{\n  \"models\": []\n}\n```\n\n**Errors:**\n- 500: Failed to connect to vLLM service or parse response\n\n**Example:**\n- GET `/api/v1/vllm/3642874/models`\n\n**Note:** \n- The vLLM service must be in \"running\" status to respond\n- Empty array may indicate the service is still initializing or has no models loaded\n- Model names can be used in the `model` parameter of the prompt endpoint","operationId":"get_vllm_models_api_v1_vllm__service_id__models_get","parameters":[{"name":"service_id","in":"path","required":true,"schema":{"type":"string","title":"Service Id"}}],"responses":{"200":{"description":"Successful Response","content":{"application/json":{"schema":{}}}},"422":{"description":"Validation Error","content":{"application/json":{"schema":{"$ref":"#/components/schemas/HTTPValidationError"}}}}}}}},"components":{"schemas":{"HTTPValidationError":{"properties":{"detail":{"items":{"$ref":"#/components/schemas/ValidationError"},"type":"array","title":"Detail"}},"type":"object","title":"HTTPValidationError"},"RecipeResponse":{"properties":{"name":{"type":"string","title":"Name"},"category":{"type":"string","title":"Category"},"description":{"type":"string","title":"Description"},"version":{"type":"string","title":"Version"},"path":{"type":"string","title":"Path"}},"type":"object","required":["name","category","description","version","path"],"title":"RecipeResponse","description":"Schema for recipe responses."},"ServiceRequest":{"properties":{"recipe_name":{"type":"string","title":"Recipe Name"},"config":{"type":"object","title":"Config","default":{}}},"type":"object","required":["recipe_name"],"title":"ServiceRequest","description":"Schema for service creation requests."},"ServiceResponse":{"properties":{"id":{"type":"string","title":"Id"},"name":{"type":"string","title":"Name"},"recipe_name":{"type":"string","title":"Recipe Name"},"status":{"type":"string","title":"Status"},"config":{"type":"object","title":"Config"},"created_at":{"type":"string","title":"Created At"}},"type":"object","required":["id","name","recipe_name","status","config","created_at"],"title":"ServiceResponse","description":"Schema for service responses."},"ValidationError":{"properties":{"loc":{"items":{"anyOf":[{"type":"string"},{"type":"integer"}]},"type":"array","title":"Location"},"msg":{"type":"string","title":"Message"},"type":{"type":"string","title":"Error Type"}},"type":"object","required":["loc","msg","type"],"title":"ValidationError"}}}}